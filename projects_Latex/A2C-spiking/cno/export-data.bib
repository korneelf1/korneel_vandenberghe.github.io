
@article{rueckauer_conversion_2017,
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	urldate = {2023-07-11},
	journal = {Frontiers in Neuroscience},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	year = {2017},
	keywords = {LI neuron is basically ReLU},
}

@article{liu_spiking_2023,
	title = {Spiking {Neural}-{Networks}-{Based} {Data}-{Driven} {Control}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/310},
	doi = {10.3390/electronics12020310},
	abstract = {Machine learning can be effectively applied in control loops to make optimal control decisions robustly. There is increasing interest in using spiking neural networks (SNNs) as the apparatus for machine learning in control engineering because SNNs can potentially offer high energy efficiency, and new SNN-enabling neuromorphic hardware is being rapidly developed. A defining characteristic of control problems is that environmental reactions and delayed rewards must be considered. Although reinforcement learning (RL) provides the fundamental mechanisms to address such problems, implementing these mechanisms in SNN learning has been underexplored. Previously, spike-timing-dependent plasticity learning schemes (STDP) modulated by factors of temporal difference (TD-STDP) or reward (R-STDP) have been proposed for RL with SNN. Here, we designed and implemented an SNN controller to explore and compare these two schemes by considering cart-pole balancing as a representative example. Although the TD-based learning rules are very general, the resulting model exhibits rather slow convergence, producing noisy and imperfect results even after prolonged training. We show that by integrating the understanding of the dynamics of the environment into the reward function of R-STDP, a robust SNN-based controller can be learned much more efficiently than TD-STDP.},
	language = {en},
	number = {2},
	urldate = {2023-07-07},
	journal = {Electronics},
	author = {Liu, Yuxiang and Pan, Wei},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Value function with SNN, control, reinforcement learning, spiking neural network},
	pages = {310},
}

@article{mei_zigzag_2021,
	title = {{ZigZag}: {Enlarging} {Joint} {Architecture}-{Mapping} {Design} {Space} {Exploration} for {DNN} {Accelerators}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {{ZigZag}},
	doi = {10.1109/TC.2021.3059962},
	abstract = {Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag's uneven mapping capabilities.},
	number = {8},
	journal = {IEEE Transactions on Computers},
	author = {Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Analytical models, Computer architecture, DNN, Hardware, Neural networks, Search engines, Search problems, Space exploration, accelerator, analytical model, dataflow, design space exploration, mapping, memory hierarchy, scheduling},
	pages = {1160--1174},
}

@misc{fedus_revisiting_2020,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	url = {https://arxiv.org/pdf/2007.06700.pdf},
	abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay -- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	month = jul,
	year = {2020},
	note = {arXiv:2007.06700 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difﬁculty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ﬁrst challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
	language = {en},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv:1506.02438 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, loss function},
}

@article{scherr_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17236-y},
	doi = {10.1038/s41467-020-17236-y},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Nature Communications},
	author = {Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models, Neuroscience, Read, Synaptic plasticity},
	pages = {3625},
}

@article{de_queiroz_reinforcement_2006,
	series = {Neural {Networks}},
	title = {Reinforcement learning of a simple control task using the spike response model},
	volume = {70},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231206002001},
	doi = {10.1016/j.neucom.2006.07.002},
	abstract = {In this work, we propose a variation of a direct reinforcement learning algorithm, suitable for usage with spiking neurons based on the spike response model (SRM). The SRM is a biologically inspired, flexible model of spiking neuron based on kernel functions that describe the effect of spike reception and emission on the membrane potential of the neuron. In our experiments, the spikes emitted by a SRM neuron are used as input signals in a simple control task. The reinforcement signal obtained from the environment is used by the direct reinforcement learning algorithm, that modifies the synaptic weights of the neuron, adjusting the spiking firing times in order to obtain a better performance at the given problem. The obtained results are comparable to those from classic methods based on value function approximation and temporal difference, for simple control tasks.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Neurocomputing},
	author = {de Queiroz, Murilo Saraiva and de Berrêdo, Roberto Coelho and de Pádua Braga, Antônio},
	month = dec,
	year = {2006},
	keywords = {Reinforcement learning, Spike response model, Spiking neuron},
	pages = {14--20},
}

@article{ding_chen_deep_2022,
	title = {Deep {Reinforcement} {Learning} with {Spiking} {Q}-learning},
	abstract = {With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of nonspiking neurons as the representation of Q-value, which can directly learn robust policies from highdimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN.},
	journal = {arXiv.org},
	author = {{Ding Chen} and {Peixi Peng} and {Tiejun Huang} and {Yonghong Tian}},
	year = {2022},
	note = {ARXIV\_ID: 2201.09754
S2ID: 0a3104f2ca2308ac9930dd57cfbbe112d04f841d},
	keywords = {read},
}

@misc{noauthor_co_nodate,
	title = {C\&{O} project - {OneDrive}},
	url = {https://tud365-my.sharepoint.com/personal/korneelvandenb_tudelft_nl//_layouts/15/onedrive.aspx?login_hint=korneelvandenb%40tudelft%2Enl&id=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project%2FRL%20of%20control%20task%20with%20spike%20response%2Epdf&parent=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project},
	urldate = {2023-06-07},
	keywords = {not read},
}

@misc{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv:1602.01783 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
}

@article{ha_world_2018,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	urldate = {2023-06-07},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10122 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{florian_reinforcement_2005,
	title = {A reinforcement learning algorithm for spiking neural networks},
	doi = {10.1109/synasc.2005.13},
	abstract = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
	journal = {Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
	author = {Florian, Răzvan V.},
	month = sep,
	year = {2005},
	doi = {10.1109/synasc.2005.13},
	note = {MAG ID: 2120905747
S2ID: cb6442b823c13339446a21fcb089428ec521a34c},
	pages = {299--306},
}

@article{bellec_biologically_2019,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets.},
	volume = {2019},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	journal = {arXiv: Neural and Evolutionary Computing},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jan,
	year = {2019},
	note = {ARXIV\_ID: 1901.09049
MAG ID: 2913168413
S2ID: ffd0f679a631b733ba2f779ce1aa0e5bbde3a8b0},
	pages = {1--37},
}

@article{liu_human-level_2022,
	title = {Human-{Level} {Control} {Through} {Directly} {Trained} {Deep} {Spiking} \${Q}\$-{Networks}},
	doi = {10.1109/tcyb.2022.3198259},
	abstract = {As the third-generation neural networks, spiking neural networks (SNNs) have great potential on neuromorphic hardware because of their high energy efficiency. However, deep spiking reinforcement learning (DSRL), that is, the reinforcement learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the nondifferentiable property of the spiking function. To address these issues, we propose a deep spiking {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DSQN) in this article. Specifically, we propose a directly trained DSRL architecture based on the leaky integrate-and-fire (LIF) neurons and deep {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DQN). Then, we adapt a direct spiking learning algorithm for the DSQN. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, generalization and energy efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly trained SNN.},
	journal = {IEEE transactions on cybernetics},
	author = {Liu, Guisong and {Wenjie Deng} and Xie, Xiurui and {Li Huang} and Tang, Huajin},
	month = jan,
	year = {2022},
	doi = {10.1109/tcyb.2022.3198259},
	pmid = {36063509},
	note = {ARXIV\_ID: 2201.07211
MAG ID: 4294691690
S2ID: 2190a17a5e937c065adc5c139b563026ac174136},
	pages = {1--12},
}

@article{luca_zanatta_artificial_2022,
	title = {Artificial versus spiking neural networks for reinforcement learning in {UAV} obstacle avoidance},
	doi = {10.1145/3528416.3530865},
	abstract = {Spiking Neural Networks (SNN) are gaining more interest from the scientific community thanks to the promise of greater energy-efficient and greater computational power. This poses several challenges as today's SNN training for RL is based on Artificial Neural Network (ANN) training and then conversion from ANN to SNN, which does not leverage SNN event-based processing inherent capabilities. The present work compares an ANN and an SNN in an event-camera-based obstacle avoidance task, trained with Reinforcement Learning (RL) using the Deep Q-Learning (DQL) algorithm. We create an experimental setup composed of Unreal Engine 4, AirSim, and an event camera that simulates a real-world obstacle avoidance environment. Additionally, we train an SNN with a gradient-based training method enabling the use of all their expressiveness even in the training phase, showing comparable performance between the ANN and the SNN. To the best of our knowledge, we are the first that implements an entire realistic pipeline with a photo-realistic simulator (Airsim) and train an SNN without converting it from a pre-trained ANN.},
	journal = {ACM International Conference on Computing Frontiers},
	author = {{Luca Zanatta} and {Francesco Barchi} and {Andrea Bartolini} and {Andrea Acquaviva}},
	month = may,
	year = {2022},
	doi = {10.1145/3528416.3530865},
	note = {MAG ID: 4229040499
S2ID: 0903d078a954427d8c875da922d52d187981b958},
}

@article{seung_learning_2003,
	title = {Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.},
	volume = {40},
	doi = {10.1016/s0896-6273(03)00761-x},
	abstract = {Abstract  It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are "hedonistic," responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
	number = {6},
	journal = {Neuron},
	author = {Seung, H. Sebastian},
	month = dec,
	year = {2003},
	doi = {10.1016/s0896-6273(03)00761-x},
	pmid = {14687542},
	note = {MAG ID: 2061897041},
	pages = {1063--1073},
}

@article{g_bellec_supplementary_2019,
	title = {Supplementary materials for: {A} solution to the learning dilemma for recurrent networks of spiking neurons},
	abstract = {S2 Optimization and regularization procedures 4 S2.1 Optimization procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.2 Firing rate regularization for LSNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.3 Weight decay regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 S2.4 Optimization with rewiring for sparse network connectivity . . . . . . . . . . . . . . . . . . 5},
	author = {{G. Bellec} and {Franz Scherr} and {Anand Subramoney} and {Elias Hajek} and {Darjan Salaj} and {R. Legenstein} and {W. Maass}},
	year = {2019},
	note = {S2ID: b3d964aa6bba3358b6921b36a19bae6454871498},
}

@article{bellec_solution_2019,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {2019},
	doi = {10.1101/738385},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. But in spite of extensive research, it has remained open how they can learn through synaptic plasticity to carry out complex network computations. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A new mathematical insight tells us how these pieces need to be combined to enable biologically plausible online  network learning through gradient descent, in particular deep reinforcement learning. This new learning method -- called e-prop -- approaches the performance of BPTT (backpropagation through time), the best known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in novel energy-efficient spike-based hardware for AI.},
	journal = {bioRxiv},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = aug,
	year = {2019},
	doi = {10.1101/738385},
	pmcid = {7367848},
	pmid = {32681001},
	note = {MAG ID: 2967417697
S2ID: 858549b00245aadc92f91a2540f01398f5f389ae},
	pages = {738385},
}

@article{kapturowski_recurrent_2019,
	title = {{RECURRENT} {EXPERIENCE} {REPLAY} {IN} {DISTRIBUTED} {REINFORCEMENT} {LEARNING}},
	abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and ﬁxed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the ﬁrst agent to exceed human-level performance in 52 of the 57 Atari games.},
	language = {en},
	author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
	year = {2019},
}

@misc{bellec_biologically_2019-1,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets},
	url = {http://arxiv.org/abs/1901.09049},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = feb,
	year = {2019},
	note = {arXiv:1901.09049 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}
