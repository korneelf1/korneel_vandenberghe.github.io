\section{Introduction} \label{section:introduction}
Spiking neural networks (SNNs) are a bio-inspired model of neural computation that use spikes to represent and process information. While SNNs have the potential for low-power event-based computation, training them effectively remains a challenge. This report presents using a reinforcement learning approach for training SNNs to control a micro aerial vehicle (MAV) during landing.

A key difference between SNNs and traditional artificial neural networks is that SNNs operate on spike sequences over time rather than activation values. This necessitates adaptations to common deep learning algorithms for training SNNs. The presented pipeline first uses supervised pre-training to help the SNN learn meaningful representations for the control task. It then fine-tunes the network using two spiking reinforcement learning algorithms: a spiking version of Deep Q-Networks (DQN) and Advantage Actor-Critic (A2C).

The spiking DQN algorithm is modified from the standard DQN to learn from interaction sequences instead of single state-transitions. A2C is also adapted for the spiking domain, again avoiding the shuffling and batching of the interactions. The trained SNN controller is then evaluated in a physics simulator and on the real system. The results provide insights into effective reinforcement learning methods for training SNNs to solve real-world control problems.