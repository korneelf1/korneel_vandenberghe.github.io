\section{Conclusion and Future Work}
This report describes the development of a pipeline that enables training spiking neural network based controllers for landing MAVs.
While the described method successfully lands an MAV, it is not without its flaws. The spiking neural network (SNN) required pre-training tasks to extract velocity information from sequences of sonar measurements. Only after this pre-training could the reinforcement learning (RL) pipeline commence. However, training SNNs with RL proved to be significantly slower than training comparable artificial neural networks (ANNs). The trained SNN exhibited numerous dead and saturated neurons—neurons that either never spike or always spike, respectively—indicative of inefficient training. This inefficiency stems from the sparse gradients within the network and the weak learning signal, which is a characteristic challenge in RL. Additionally, the discrete set of output actions was suboptimal for control tasks. 

For future work, there are several avenues I would like to explore. First, I plan to investigate the application of findings from the original R2D2 publication \cite{kapturowski2018recurrent_r2d2}, particularly those related to recurrent replay buffers, to an off-policy, policy gradient reinforcement learning algorithm. This could potentially eliminate the need for pre-training. Next, I aim to combine this approach with an asymmetric actor-critic network \cite{pinto2017asymmetric}. In this setup, the actor, an SNN deployed on the real MAV, would receive the inputs observed by the MAV, while the critic, an ANN, would process all states available in the simulation and, optionally, explicitly receive a state history at every forward pass. This combination could enable faster and more stable training.

Another interesting direction is to explore stochastic neuron models that become more deterministic as training progresses. This approach could encourage greater network activity and address the issue of dead and saturated neurons. Additionally, balancing exploration and exploitation during training is crucial in reinforcement learning. This balance could be inherently encoded in the SNN agent, simplifying the training pipeline and potentially leading to more efficient training.