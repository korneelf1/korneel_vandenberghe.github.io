\section{Related work} \label{section:lit_review} 
Existing research in the field of neuromorphic Reinforcement Learning (RL) has mainly applied one algorithm; deep Q learning (DQN). The deep Q algorithm is a well established algorithm, introduced by DeepMind. The original publication demonstrated the performance of this algorithm on multiple Atari games, displaying its versatility~\cite{mnih2013playing}. It is a simple and elegant algorithm, based on tabular Q-learning, that uses an artificial neural network as policy. One component, important in the performance of this algorithm, is the experience replay~\cite{fedus_revisiting_2020}. During the training phase, experience from previous environment interactions is used together with the most recent experience to avoid bias in the training data and improve training characteristics. The spiking variant of this algorithm has been demonstrated in earlier work. DSQN has been applied on the Atari environments and on the Airsim environment and outperform several ANN based solutions~\cite{DSQN_Atari, DQN_Atari_human-level_2022, DSQN_AirSim}. One drawback of current implementations is that temporal information is not exploited. They use rate encoding techniques, running several forward passes per input, decreasing efficiency and undermining the temporal learning capabilities of SNN. The implementations used in this report make use of sequences for training where a linear layer is used for encoding sensor data to the spiking domain, closely resembling a population encoding mechanism. 

Neuromorphic control has been applied on Micro Aerial Vehicles (MAV) \cite{Stroobants2022DesignProcessors, Stroobants2022NeuromorphicQuadrotors, Vitale2021Event-drivenChip, Paredes-Valles2023FullyFlight}, aiming to exploit the promises of energy efficiency and latency from neuromorphic algorithms. This stream of work spans manual tuning to supervised training methods for developping low level controllers, to full end to end control. A neuromorphic PID controller was manually tuned and demonstrated for onboard attitude control \cite{Stroobants2022DesignProcessors}. Next to control, an SNN has been successfully used for attitude estimation in highly dynamic environments, using IMU data \cite{Stroobants2022NeuromorphicQuadrotors}. End-to-end control was later demonstrated on the Intel Loihi chip \cite{Davies2018Loihi:Learning}. The network used optical flow, trained in an unsupervised setting, for vertical control of an MAV \cite{Paredes-Valles2023FullyFlight}.

% The previous work on SNN based RL algorithms span a wide array of different environments, tasks and training methods. Early research already harnessed bio-inspired training algorithms such as spike-timing-dependent plasticity (STDP)~\cite{STDP_survey}. STDP is commonly described as: \textit{cells that fire together, wire together}. It is a concept derived from neuroscience, that assumes that neurons that often spike simultaneously, probably represent the same information. R.V. Florian\cite{florian_reinforcement_2005} proposed an algorithm based on deep Q learning, modulating the STDP with the global reward signal, for a bio-inspired problem. The agent controls a worm that solves a localization problem based on gradient descent, receiving a positive reward if it comes closer to the source, while receiving a negative reward when moving to a region with a lower concentration. Further work shows that using reward modulated spike-timing-dependent plasticity (R-STDP) or temporal difference spike-timing-dependent plasticity (TD-STDP) allows networks to train more complex tasks such as the CartPole~\cite{liu_spiking_2023}, a task where the model has to balance a stick on a cart, by only applying a force to the cart~\cite{CartPole}. However, the model showed slow convergence and noisy, imperfect results. Another method inspired by neuroscience is eProp. This method showed to approach the performance of backpropagation through time for recurrent SNN~\cite{eProp_pong}.
\\
% Methods that more closely resemble the training of non-spiking networks, include shadow training and surrogate gradient training. In shadow training~\cite{ding2021optimal}. We train an ANN and convert it to an SNN based agent. This approach, however, has shown to produce worse performing models compared to the ANN based equivalent and to a DQN algorithm directly optimizing the SNN based agent (DSQN)\cite{ding_chen_deep_2022}. This is explained by the fact that the converted SNN will always be limited by the ANN from which it is converted, while the other two methods (DQN and DSQN) train directly from the environment. Surrogate gradient training~\cite{neftci2019surrogate} overcomes the non-differentiable of spikes by modelling the step function that presents the spiking behaviour of a neuron, with a differentiable surrogate function that approaches the step function the backward pass.
