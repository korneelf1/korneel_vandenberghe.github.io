@inproceedings{
kapturowski2018recurrent_r2d2,
title={Recurrent Experience Replay in Distributed Reinforcement Learning},
author={Steven Kapturowski and Georg Ostrovski and Will Dabney and John Quan and Remi Munos},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=r1lyTjAqYX},
}
@article{pinto2017asymmetric,
  title={Asymmetric actor critic for image-based robot learning},
  author={Pinto, Lerrel and Andrychowicz, Marcin and Welinder, Peter and Zaremba, Wojciech and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.06542},
  year={2017}
}

@inproceedings{neuman_robomorphic_2021,
	address = {Virtual USA},
	title = {Robomorphic computing: a design methodology for domain-specific accelerators parameterized by robot morphology},
	isbn = {978-1-4503-8317-2},
	shorttitle = {Robomorphic computing},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446746},
	doi = {10.1145/3445814.3446746},
	language = {en},
	urldate = {2023-08-08},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Neuman, Sabrina M. and Plancher, Brian and Bourgeat, Thomas and Tambe, Thierry and Devadas, Srinivas and Reddi, Vijay Janapa},
	month = apr,
	year = {2021},
	pages = {674--686},
}
@article{ding2021optimal,
  title={Optimal ann-snn conversion for fast and accurate inference in deep spiking neural networks},
  author={Ding, Jianhao and Yu, Zhaofei and Tian, Yonghong and Huang, Tiejun},
  journal={arXiv preprint arXiv:2105.11654},
  year={2021}
}

@ARTICLE{CartPole,
  author={Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Neuronlike adaptive elements that can solve difficult learning control problems}, 
  year={1983},
  volume={SMC-13},
  number={5},
  pages={834-846},
  doi={10.1109/TSMC.1983.6313077}}
@article{Christensen20222022Engineering,
    title = {{2022 roadmap on neuromorphic computing and engineering}},
    year = {2022},
    journal = {Neuromorphic Computing and Engineering },
    author = {Christensen, Dennis V. and Dittmann, Regina and Linares-Barranco, Bernabe and Sebastian, Abu and Le Gallo, Manuel and Redaelli, Andrea and Slesazeck, Stefan and Mikolajick, Thomas and Spiga, Sabina and Menzel, Stephan and Valov, Ilia and Milano, Gianluca and Ricciardi, Carlo and Liang, Shi Jun and Miao, Feng and Lanza, Mario and Quill, Tyler J. and Keene, Scott T. and Salleo, Alberto and Grollier, Julie and Markovi{\'{c}}, Danijela and Mizrahi, Alice and Yao, Peng and Yang, J. Joshua and Indiveri, Giacomo and Strachan, John Paul and Datta, Suman and Vianello, Elisa and Valentian, Alexandre and Feldmann, Johannes and Li, Xuan and Pernice, Wolfram H.P. and Bhaskaran, Harish and Furber, Steve and Neftci, Emre and Scherr, Franz and Maass, Wolfgang and Ramaswamy, Srikanth and Tapson, Jonathan and Panda, Priyadarshini and Kim, Youngeun and Tanaka, Gouhei and Thorpe, Simon and Bartolozzi, Chiara and Cleland, Thomas A. and Posch, Christoph and Liu, Shih Chii and Panuccio, Gabriella and Mahmud, Mufti and Mazumder, Arnab Neelim and Hosseini, Morteza and Mohsenin, Tinoosh and Donati, Elisa and Tolu, Silvia and Galeazzi, Roberto and Christensen, Martin Ejsing and Holm, Sune and Ielmini, Daniele and Pryds, N.},
    number = {2},
    month = {5},
    pages = {022501},
    volume = {2},
    publisher = {IOP Publishing},
    url = {https://iopscience.iop.org/article/10.1088/2634-4386/ac4a83 https://iopscience.iop.org/article/10.1088/2634-4386/ac4a83/meta},
    doi = {10.1088/2634-4386/AC4A83},
    issn = {2634-4386},
    arxivId = {2105.05956},
    keywords = {convolutional neural networks, deep learning, memristor, neuromorphic computation, robotics, self-driving cars, spiking neural networks}
}

@article{SubbulakshmiRadhakrishnan2021ANetwork,
    title = {{A biomimetic neural encoder for spiking neural network}},
    year = {2021},
    journal = {Nature Communications 2021 12:1},
    author = {Subbulakshmi Radhakrishnan, Shiva and Sebastian, Amritanand and Oberoi, Aaryan and Das, Sarbashis and Das, Saptarshi},
    number = {1},
    month = {4},
    pages = {1--10},
    volume = {12},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/s41467-021-22332-8},
    doi = {10.1038/s41467-021-22332-8},
    issn = {2041-1723},
    pmid = {33837210},
    keywords = {Electronic devices, Sensors and biosensors, Two, dimensional materials}
}

@article{Mirhoseini2021ADesign,
    title = {{A graph placement methodology for fast chip design}},
    year = {2021},
    journal = {Nature |},
    author = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
    volume = {594},
    url = {https://doi.org/10.1038/s41586-021-03544-w},
    doi = {10.1038/s41586-021-03544-w}
}

@article{Oikonomou2023AReaching,
    title = {{A Hybrid Reinforcement Learning Approach With a Spiking Actor Network for Efficient Robotic Arm Target Reaching}},
    year = {2023},
    journal = {IEEE Robotics and Automation Letters},
    author = {Oikonomou, Katerina Maria and Kansizoglou, Ioannis and Gasteratos, Antonios},
    number = {5},
    month = {5},
    pages = {3007--3014},
    volume = {8},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    doi = {10.1109/LRA.2023.3264836},
    issn = {23773766},
    keywords = {Bioinspired robot learning, deep deterministic policy gradient, reinforcement learning, robotic arm target reach, spiking neural networks}
}

@article{Wang2022AReinforcement,
    title = {{A neural network model for timing control with reinforcement}},
    year = {2022},
    journal = {Frontiers in computational neuroscience},
    author = {Wang, Jing and El-Jayyousi, Yousuf and Ozden, Ilker},
    month = {10},
    volume = {16},
    publisher = {Front Comput Neurosci},
    url = {https://pubmed.ncbi.nlm.nih.gov/36277612/},
    doi = {10.3389/FNCOM.2022.918031},
    issn = {1662-5188},
    pmid = {36277612},
    keywords = {Ilker Ozden, Jing Wang, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PMC9579423, PubMed Abstract, Yousuf El-Jayyousi, doi:10.3389/fncom.2022.918031, pmid:36277612}
}

@article{Hodgkin1952ANerve,
    title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
    year = {1952},
    journal = {The Journal of Physiology},
    author = {Hodgkin, A. L. and Huxley, A. F.},
    number = {4},
    month = {8},
    pages = {500--544},
    volume = {117},
    doi = {10.1113/JPHYSIOL.1952.SP004764},
    issn = {14697793},
    pmid = {12991237}
}

@article{Florian2005ANetworks,
    title = {{A reinforcement learning algorithm for spiking neural networks}},
    year = {2005},
    journal = {Proceedings - Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2005},
    author = {Florian, Rǎzvan V.},
    pages = {299--306},
    volume = {2005},
    publisher = {IEEE Computer Society},
    isbn = {0769524532},
    doi = {10.1109/SYNASC.2005.13}
}

@article{Florian2005ANetworksb,
    title = {{A reinforcement learning algorithm for spiking neural networks}},
    year = {2005},
    journal = {Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
    author = {Florian, Răzvan V},
    pages = {299--306},
    doi = {10.1109/synasc.2005.13}
}

@article{Bellec2019ANeurons,
    title = {{A solution to the learning dilemma for recurrent networks of spiking neurons}},
    year = {2019},
    journal = {bioRxiv},
    author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
    pages = {738385},
    volume = {2019},
    doi = {10.1101/738385}
}

@article{Scherr2020ANeurons,
    title = {{A solution to the learning dilemma for recurrent networks of spiking neurons}},
    year = {2020},
    journal = {Nature Communications},
    author = {Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
    number = {1},
    pages = {3625},
    volume = {11},
    url = {https://www.nature.com/articles/s41467-020-17236-y https://www.nature.com/articles/s41467-020-17236-y.pdf},
    doi = {10.1038/s41467-020-17236-y},
    issn = {2041-1723},
    keywords = {Electrical and electronic engineering, Learning algorithms, Network models, Neuroscience, Read, Synaptic plasticity},
    language = {en}
}

@article{Auge2021ANetworks,
    title = {{A Survey of Encoding Techniques for Signal Processing in Spiking Neural Networks}},
    year = {2021},
    journal = {Neural Processing Letters},
    author = {Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
    number = {6},
    month = {12},
    pages = {4693--4710},
    volume = {53},
    publisher = {Springer},
    url = {https://link.springer.com/article/10.1007/s11063-021-10562-2},
    doi = {10.1007/S11063-021-10562-2/TABLES/1},
    issn = {1573773X},
    keywords = {Neural coding, Neuromorphic computing, Rate coding, Spiking neural networks, Temporal coding}
}

@article{Bing2018ANetworks,
    title = {{A Survey of Robotics Control Based on Learning-Inspired Spiking Neural Networks}},
    year = {2018},
    journal = {Frontiers in Neurorobotics},
    author = {Bing, Zhenshan and Meschede, Claus and R{\"{o}}hrbein, Florian and Huang, Kai and Knoll, Alois C},
    volume = {12},
    url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00035 https://www.frontiersin.org/articles/10.3389/fnbot.2018.00035/pdf},
    doi = {10.3389/fnbot.2018.00035},
    issn = {1662-5218},
    keywords = {Brain-inspired robotics, Learning control, Spiking Neural network, Survey, neurorobotics},
    language = {English}
}

@article{Zanatta2022ArtificialAvoidance,
    title = {{Artificial versus spiking neural networks for reinforcement learning in UAV obstacle avoidance}},
    year = {2022},
    journal = {ACM International Conference on Computing Frontiers},
    author = {Zanatta, Luca and Barchi, Francesco and Bartolini, Andrea and Acquaviva, Andrea},
    doi = {10.1145/3528416.3530865}
}

@article{Mnih2016AsynchronousLearning,
    title = {{Asynchronous Methods for Deep Reinforcement Learning}},
    year = {2016},
    author = {Mnih, Volodymyr and Puigdom{\`{e}}nech Badia, Adrià and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
    isbn = {1602.01783v2},
    arxivId = {1602.01783v2}
}

@misc{Bellec2019BiologicallyNets,
    title = {{Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets}},
    year = {2019},
    author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
    publisher = {arXiv},
    url = {http://arxiv.org/abs/1901.09049 https://arxiv.org/pdf/1901.09049.pdf https://arxiv.org/abs/1901.09049},
    keywords = {Computer Science - Neural and Evolutionary Computing}
}

@article{Bellec2019BiologicallyNets.,
    title = {{Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets.}},
    year = {2019},
    journal = {arXiv: Neural and Evolutionary Computing},
    author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
    pages = {1--37},
    volume = {2019}
}

@article{Miconi2017BiologicallyTasks,
    title = {{Biologically plausible learning in recurrent neural networks reproduces neural dynamics observed during cognitive tasks}},
    year = {2017},
    journal = {eLife},
    author = {Miconi, Thomas},
    editor = {Frank, Michael J},
    pages = {e20899},
    volume = {6},
    url = {https://doi.org/10.7554/eLife.20899 https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvMjA4OTkvZWxpZmUtMjA4OTktdjIucGRmP2Nhbm9uaWNhbFVyaT1odHRwczovL2VsaWZlc2NpZW5jZXMub3JnL2FydGljbGVzLzIwODk5/elife-20899-v2.pdf?_hash=W0aFhuHQVJNohAxQoL1eTVP%2FdM48mukaN%2F4B2YaDiFc%3D},
    doi = {10.7554/eLife.20899},
    issn = {2050-084X},
    keywords = {cognition, computational neuroscience, learning, modeling, recurrent neural networks}
}

@article{Backus1978CanStyle,
    title = {{Can programming be liberated from the von Neumann style?}},
    year = {1978},
    journal = {Communications of the ACM},
    author = {Backus, John},
    number = {8},
    month = {8},
    pages = {613--641},
    volume = {21},
    publisher = {ACMPUB27New York, NY, USA},
    url = {https://dl.acm.org/doi/10.1145/359576.359579},
    doi = {10.1145/359576.359579},
    issn = {15577317},
    keywords = {algebra of programs, applicative computing systems, applicative state transition systems, combining forms, functional forms, functional programming, metacomposition, models of computing systems, program correctness, program termination, program transformation, programming languages, von Neumann computers, von Neumann languages}
}

@article{Kaufmann2023Champion-levelLearning,
    title = {{Champion-level drone racing using deep reinforcement learning}},
    year = {2023},
    journal = {Nature},
    author = {Kaufmann, Elia and Bauersfeld, Leonard and Loquercio, Antonio and M{\"{u}}ller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
    number = {7976},
    pages = {982--987},
    volume = {620},
    url = {https://www.nature.com/articles/s41586-023-06419-4 https://www.nature.com/articles/s41586-023-06419-4.pdf},
    doi = {10.1038/s41586-023-06419-4},
    issn = {1476-4687},
    keywords = {Aerospace engineering, Computer science, Electrical and electronic engineering, Mechanical engineering},
    language = {en}
}

@article{Schoepe2023Closed-loopSystems,
    title = {{Closed-loop sound source localization in neuromorphic systems}},
    year = {2023},
    journal = {Neuromorphic Computing and Engineering},
    author = {Schoepe, Thorben and Gutierrez-Galan, Daniel and Dominguez-Morales, Juan P and Greatorex, Hugh and Jimenez-Fernandez, Angel and Linares-Barranco, Alejandro and Chicca, Elisabetta},
    number = {2},
    pages = {024009},
    volume = {3},
    url = {https://iopscience.iop.org/article/10.1088/2634-4386/acdaba https://pure.rug.nl/ws/portalfiles/portal/689382619/Schoepe_2023_Neuromorph._Comput._Eng._3_024009.pdf},
    doi = {10.1088/2634-4386/acdaba},
    issn = {2634-4386},
    language = {en}
}

@article{OdonoghueCOMBININGQ-LEARNING,
    title = {{COMBINING POLICY GRADIENT AND Q-LEARNING}},
    author = {O'donoghue, Brendan and Munos, Rémi and Kavukcuoglu, Koray and Volodymyr, & and Deepmind, Mnih},
    arxivId = {1611.01626v3}
}

@article{Chevtchenko2021CombiningRewards,
    title = {{Combining STDP and binary networks for reinforcement learning from images and sparse rewards}},
    year = {2021},
    journal = {Neural Networks},
    author = {Chevtchenko, Sérgio F. and Ludermir, Teresa B.},
    month = {12},
    pages = {496--506},
    volume = {144},
    publisher = {Pergamon},
    doi = {10.1016/J.NEUNET.2021.09.010},
    issn = {0893-6080},
    pmid = {34601362},
    keywords = {Binary neural networks, Reinforcement learning, STDP, Spiking neural networks}
}

@article{PotjansCommunicatedAgent,
    title = {{Communicated by Ron Meir A Spiking Neural Network Model of an Actor-Critic Learning Agent}},
    author = {Potjans, Wiebke and Morrison, Abigail and Diesmann, Markus},
    url = {http://direct.mit.edu/neco/article-pdf/21/2/301/1060601/neco.2008.08-07-593.pdf}
}

@article{Song2000CompetitivePlasticity,
    title = {{Competitive Hebbian learning through spike-timing-dependent synaptic plasticity}},
    year = {2000},
    journal = {Nature Neuroscience 2000 3:9},
    author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
    number = {9},
    month = {9},
    pages = {919--926},
    volume = {3},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/nn0900_919},
    doi = {10.1038/78829},
    issn = {1546-1726},
    pmid = {10966623},
    keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general}
}

@article{Rueckauer2017ConversionClassification,
    title = {{Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification}},
    year = {2017},
    journal = {Frontiers in Neuroscience},
    author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
    volume = {11},
    url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682 https://www.frontiersin.org/articles/10.3389/fnins.2017.00682/pdf},
    issn = {1662-453X},
    keywords = {LI neuron is basically ReLU}
}

@article{Hausknecht2015DeepMDPs,
    title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
    year = {2015},
    author = {Hausknecht, Matthew and Stone, Peter},
    url = {www.aaai.org},
    keywords = {Technical Report FS-15-06}
}

@article{Tang2020DeepControl,
    title = {{Deep Reinforcement Learning with Population-Coded Spiking Neural Network for Continuous Control}},
    year = {2020},
    author = {Tang, Guangzhi and Kumar, Neelesh and Yoo, Raymond and Michmizos, Konstantinos P.},
    month = {10},
    url = {http://arxiv.org/abs/2010.09635},
    arxivId = {2010.09635}
}

@article{Chen2022DeepQ-learning,
    title = {{Deep Reinforcement Learning with Spiking Q-learning}},
    year = {2022},
    author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
    month = {1},
    arxivId = {2201.09754}
}

@inproceedings{Stroobants2022DesignProcessors,
    title = {{Design and implementation of a parsimonious neuromorphic PID for onboard altitude control for MAVs using neuromorphic processors}},
    year = {2022},
    booktitle = {ICONS: International Conference on Neuromorphic Systems},
    author = {Stroobants, Stein and Dupeyroux, Julien and De Croon, Guido},
    pages = {1--7},
    publisher = {ACM},
    url = {https://dl.acm.org/doi/10.1145/3546790.3546799 https://dl.acm.org/doi/pdf/10.1145/3546790.3546799},
    isbn = {978-1-4503-9789-6},
    doi = {10.1145/3546790.3546799},
    language = {en}
}

@article{Zanatta2023Directly-trainedAccelerator,
    title = {{Directly-trained Spiking Neural Networks for Deep Reinforcement Learning: Energy efficient implementation of event-based obstacle avoidance on a neuromorphic accelerator}},
    year = {2023},
    journal = {Neurocomputing},
    author = {Zanatta, Luca and Di Mauro, Alfio and Barchi, Francesco and Bartolini, Andrea and Benini, Luca and Acquaviva, Andrea},
    month = {12},
    pages = {126885},
    volume = {562},
    publisher = {Elsevier},
    doi = {10.1016/J.NEUCOM.2023.126885},
    issn = {0925-2312},
    keywords = {DQN, Neuromorphic computing, Neuromorphic hardware, Reinforcement learning, Spiking Neural Networks, UAV}
}

@article{Croon2023Drone-racingAI,
    title = {{Drone-racing champions outpaced by AI}},
    year = {2023},
    journal = {Nature},
    author = {Croon, De and E, Guido C H},
    number = {7976},
    pages = {952--954},
    volume = {620},
    url = {https://www.nature.com/articles/d41586-023-02506-8 https://www.nature.com/articles/d41586-023-02506-8.pdf https://www.nature.com/articles/d41586-023-02506-8},
    doi = {10.1038/d41586-023-02506-8},
    keywords = {Engineering, Machine learning},
    language = {en}
}

@article{Gerstner2018EligibilityRules,
    title = {{Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules}},
    year = {2018},
    journal = {Frontiers in Neural Circuits},
    author = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
    month = {7},
    pages = {350307},
    volume = {12},
    publisher = {Frontiers Media S.A.},
    doi = {10.3389/FNCIR.2018.00053/BIBTEX},
    issn = {16625110},
    pmid = {30108488},
    arxivId = {1801.05219},
    keywords = {Behavioral learning, Eligibility trace, Hebb rule, Neuromodulators, Reinforcement learning, Surprise, Synaptic plasticity, Synaptic tagging}
}

@inproceedings{Rebecq2018ESIM:Simulator,
    title = {{ESIM: an Open Event Camera Simulator}},
    shorttitle = {ESIM},
    year = {2018},
    booktitle = {Conference on Robot Learning},
    author = {Rebecq, Henri and Gehrig, Daniel and Scaramuzza, Davide},
    pages = {969--982},
    publisher = {PMLR},
    url = {https://proceedings.mlr.press/v87/rebecq18a.html http://proceedings.mlr.press/v87/rebecq18a/rebecq18a.pdf},
    language = {en}
}

@inproceedings{Maqueda2018Event-BasedCars,
    title = {{Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars}},
    year = {2018},
    booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
    pages = {5419--5427},
    publisher = {IEEE},
    url = {https://ieeexplore.ieee.org/document/8578666/ https://openaccess.thecvf.com/content_cvpr_2018/papers/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.pdf},
    isbn = {978-1-5386-6420-9},
    doi = {10.1109/CVPR.2018.00568},
    language = {en}
}

@article{Gallego2022Event-BasedSurvey,
    title = {{Event-Based Vision: A Survey}},
    shorttitle = {Event-Based Vision},
    year = {2022},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    author = {Gallego, Guillermo and Delbr{\"{u}}ck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J and Conradt, Jörg and Daniilidis, Kostas and Scaramuzza, Davide},
    number = {1},
    pages = {154--180},
    volume = {44},
    url = {https://ieeexplore.ieee.org/ielx7/34/9639876/09138762.pdf?tp=&arnumber=9138762&isnumber=9639876&ref= https://ieeexplore.ieee.org/document/9138762/?arnumber=9138762},
    doi = {10.1109/TPAMI.2020.3008413},
    issn = {1939-3539},
    keywords = {Brightness, Cameras, Event cameras, Retina, Robot vision systems, Voltage control, asynchronous sensor, bio-inspired vision, high dynamic range, low latency, low power}
}

@inproceedings{Vitale2021Event-drivenChip,
    title = {{Event-driven Vision and Control for UAVs on a Neuromorphic Chip}},
    year = {2021},
    booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
    author = {Vitale, Antonio and Renner, Alpha and Nauer, Celine and Scaramuzza, Davide and Sandamirskaya, Yulia},
    pages = {103--109},
    publisher = {IEEE},
    url = {https://ieeexplore.ieee.org/document/9560881/ https://rpg.ifi.uzh.ch/docs/ICRA21_Vitale.pdf},
    isbn = {978-1-72819-077-8},
    doi = {10.1109/ICRA48506.2021.9560881},
    language = {en}
}

@article{Zhu2022Evolutionary,
    title = {{Evolutionary vs imitation learning for neuromorphic control at the edge * You may also like CMOS-compatible neuromorphic devices for neuromorphic perception and computing: a review Evolutionary vs imitation learning for neuromorphic control at the edge *}},
    year = {2022},
    author = {Zhu, Yixin and Mao, Huiwu and Zhu, Ying and -, al and Schuman, Catherine and Patton, Robert and Kulkarni, Shruti and Parsa, Maryam and Stahl, Christopher and Quentin Haas, N and Parker Mitchell, J and Snyder, Shay and Nagle, Amelie and Shanafield, Alexandra and Potok, Thomas},
    url = {https://doi.org/10.1088/2634-4386/ac45e7},
    doi = {10.1088/2634-4386/ac45e7},
    keywords = {autonomous driving, evolutionary optimization, imitation learning, spiking neural networks}
}

@article{BurgersEvolvingBlimps,
    title = {{Evolving Spiking Neural Networks to Mimic PID Control for Autonomous Blimps}},
    author = {Burgers, Tim and Stroobants, Stein and De Croon, Guido C H E},
    arxivId = {2309.12937v1}
}

@article{LuEvolving-to-LearnNetworks,
    title = {{Evolving-to-Learn Reinforcement Learning Tasks with Spiking Neural Networks}},
    author = {Lu, J and Hagenaars, J J and De Croon, G C H E},
    arxivId = {2202.12322v1}
}

@article{Mueggler2017FastDetection,
    title = {{Fast event-based corner detection}},
    year = {2017},
    author = {Mueggler, Elias and Bartolozzi, Chiara and Scaramuzza, Davide},
    pages = {1--8},
    doi = {10.5167/uzh-138925}
}

@article{DiehlFast-classifyingBalancing,
    title = {{Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing}},
    journal = {ieeexplore.ieee.orgPU Diehl, D Neil, J Binas, M Cook, SC Liu, M Pfeiffer2015 International joint conference on neural networks (IJCNN), 2015•ieeexplore.ieee.org},
    author = {Diehl, PU and Neil, D and Binas, J and {\ldots}, M Cook - … joint conference on and 2015, undefined},
    url = {https://ieeexplore.ieee.org/abstract/document/7280696/}
}

@misc{Paredes-Valles2023FullyFlight,
    title = {{Fully neuromorphic vision and control for autonomous drone flight}},
    year = {2023},
    author = {Paredes-Vall{\'{e}}s, Federico and Hagenaars, Jesse and Dupeyroux, Julien and Stroobants, Stein and Xu, Yingfu and de Croon, Guido},
    publisher = {arXiv},
    url = {http://arxiv.org/abs/2303.08778 https://arxiv.org/pdf/2303.08778.pdf https://arxiv.org/abs/2303.08778},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics}
}

@article{Chen2024FullyLearning,
    title = {{Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning}},
    year = {2024},
    author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
    month = {1},
    arxivId = {2401.05444}
}

@inproceedings{Ambrose2020GRANT:Targeter,
    title = {{GRANT: Ground-Roaming Autonomous Neuromorphic Targeter}},
    shorttitle = {GRANT},
    year = {2020},
    booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
    author = {Ambrose, Jonathan D and Foshie, Adam Z and Dean, Mark E and Plank, James S and Rose, Garrett S and Mitchell, J Parker and Schuman, Catherine D and Bruer, Grant},
    pages = {1--8},
    publisher = {IEEE},
    url = {https://ieeexplore.ieee.org/document/9207276/ https://www.osti.gov/servlets/purl/1671407},
    isbn = {978-1-72816-926-2},
    doi = {10.1109/IJCNN48605.2020.9207276},
    language = {en}
}

@misc{Schulman2018High-DimensionalEstimation,
    title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
    year = {2018},
    author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
    publisher = {arXiv},
    url = {http://arxiv.org/abs/1506.02438 https://arxiv.org/pdf/1506.02438.pdf},
    keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, loss function},
    language = {en}
}

@article{Mnih2015Human-levelLearning,
    title = {{Human-level control through deep reinforcement learning}},
    year = {2015},
    journal = {Nature 2015 518:7540},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    number = {7540},
    month = {2},
    pages = {529--533},
    volume = {518},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/nature14236},
    doi = {10.1038/nature14236},
    issn = {1476-4687},
    pmid = {25719670},
    keywords = {Computer science}
}

@article{LiuHuman-LevelQ-Networks,
    title = {{Human-Level Control through Directly-Trained Deep Spiking Q-Networks}},
    author = {Liu, Guisong and Deng, Wenjie and Xie, Xiurui and Huang, Li and Tang, Huajin},
    arxivId = {2201.07211v3},
    keywords = {Atari Games, Directly-Training, Index Terms-Deep Reinforcement Learning, Spiking Neural Networks}
}

@misc{IEEEPDF:,
    title = {{IEEE Xplore Full-Text PDF:}},
    url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9053856#page=3.59}
}

@article{Patel2019ImprovedGame,
    title = {{Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game}},
    year = {2019},
    author = {Patel, Devdhar and Hazan, Hananel and Saunders, Daniel J and Siegelmann, Hava T and Kozma, Robert},
    arxivId = {1903.11012v3},
    keywords = {*, Corresponding author}
}

@article{Bing2020IndirectVehicle,
    title = {{Indirect and direct training of spiking neural networks for end-to-end control of a lane-keeping vehicle}},
    year = {2020},
    journal = {Neural Networks},
    author = {Bing, Zhenshan and Meschede, Claus and Chen, Guang and Knoll, Alois and Huang, Kai},
    pages = {21--36},
    volume = {121},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608019301595 https://arxiv.org/pdf/2003.04603 https://www.sciencedirect.com/science/article/pii/S0893608019301595},
    doi = {10.1016/j.neunet.2019.05.019},
    issn = {0893-6080},
    keywords = {End-to-end learning, Lane keeping, R-STDP}
}

@misc{Levy2021InnaterasAI,
    title = {{Innatera's Spiking Neural Processor - Brain-like architecture targets ultra-low power AI}},
    year = {2021},
    booktitle = {Linley Microprocessor Report},
    author = {Levy, Markus},
    howpublished = {https://www.innatera.com/innatera-mpr-2021.pdf}
}

@article{GuInterpolatedLearning,
    title = {{Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning}},
    author = {Gu, Shixiang and Deepmind, Timothy Lillicrap and Ghahramani, Zoubin and Turner, Richard E and Sch{\"{o}}lkopf, Bernhard and Levine, Sergey},
    arxivId = {1706.00387v1}
}

@article{Rosenfeld2019LearningGradients,
    title = {{Learning First-to-Spike Policies for Neuromorphic Control Using Policy Gradients}},
    year = {2019},
    journal = {IEEE Workshop on Signal Processing Advances in Wireless Communications, SPAWC},
    author = {Rosenfeld, Bleema and Simeone, Osvaldo and Rajendran, Bipin},
    month = {7},
    volume = {2019-July},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    isbn = {9781538665282},
    doi = {10.1109/SPAWC.2019.8815546},
    arxivId = {1810.09977},
    keywords = {Neuromorphic Computing, Policy Gradient, Reinforcement Learning, Spiking Neural Network}
}

@article{Xie2004LearningSpiking,
    title = {{Learning in neural networks by reinforcement of irregular spiking}},
    year = {2004},
    journal = {Physical Review E},
    author = {Xie, Xiaohui and Seung, H Sebastian},
    number = {4},
    pages = {041909},
    volume = {69},
    url = {https://link.aps.org/doi/10.1103/PhysRevE.69.041909 https://web.archive.org/web/20060906214311id_/http://seunglab.mit.edu/people/seung/papers/XieSeung2004.pdf},
    doi = {10.1103/PhysRevE.69.041909},
    issn = {1539-3755, 1550-2376},
    language = {en}
}

@article{Seung2003LearningTransmission.,
    title = {{Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.}},
    year = {2003},
    journal = {Neuron},
    author = {Seung, H Sebastian},
    number = {6},
    pages = {1063--1073},
    volume = {40},
    doi = {10.1016/s0896-6273(03)00761-x}
}

@article{Rumelhart1986LearningErrors,
    title = {{Learning representations by back-propagating errors}},
    year = {1986},
    journal = {Nature 1986 323:6088},
    author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    number = {6088},
    pages = {533--536},
    volume = {323},
    publisher = {Nature Publishing Group},
    url = {https://www.nature.com/articles/323533a0},
    doi = {10.1038/323533a0},
    issn = {1476-4687},
    keywords = {Humanities and Social Sciences, Science, multidisciplinary}
}

@article{Davies2018Loihi:Learning,
    title = {{Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}},
    year = {2018},
    journal = {IEEE Micro},
    author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
    number = {1},
    pages = {82--99},
    volume = {38},
    doi = {https://doi.org/10.1109/MM.2018.112130359}
}

@article{Perez-CarrascoMappingConvNets,
    title = {{Mapping from frame-driven to frame-free event-driven vision systems by low-rate rate coding and coincidence processing--application to feedforward ConvNets}},
    journal = {ieeexplore.ieee.orgJA P{\'{e}}rez-Carrasco, B Zhao, C Serrano, B Acha, T Serrano-Gotarredona, S ChenIEEE transactions on pattern analysis and machine intelligence, 2013•ieeexplore.ieee.org},
    author = {P{\'{e}}rez-Carrasco, JA and Zhao, B and {\ldots}, C Serrano - IEEE transactions on and 2013, undefined},
    url = {https://ieeexplore.ieee.org/abstract/document/6497055/}
}

@article{Silver2017MasteringAlgorithm,
    title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}},
    year = {2017},
    author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
    month = {12},
    url = {https://arxiv.org/abs/1712.01815v1},
    arxivId = {1712.01815}
}

@article{HeessMemory-basedNetworks,
    title = {{Memory-based control with recurrent neural networks}},
    author = {Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
    arxivId = {1512.04455v1}
}

@article{Schmidgall2023Meta-SpikePropamine:Networks,
    title = {{Meta-SpikePropamine: learning to learn with synaptic plasticity in spiking neural networks}},
    year = {2023},
    journal = {Frontiers in neuroscience},
    author = {Schmidgall, Samuel and Hays, Joe},
    volume = {17},
    publisher = {Front Neurosci},
    url = {https://pubmed.ncbi.nlm.nih.gov/37250397/},
    doi = {10.3389/FNINS.2023.1183321},
    issn = {1662-4548},
    pmid = {37250397},
    keywords = {Joe Hays, MEDLINE, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, PMC10213417, PubMed Abstract, Samuel Schmidgall, doi:10.3389/fnins.2023.1183321, pmid:37250397}
}

@article{Rafailov:MOTO:Learning,
    title = {{MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning}},
    author = {Rafailov˚:, Rafael and Kyle, Rafailov˚: and Hatch˚:, Hatch˚: and Kolev, Victor and Martin, John D and Phielipp, Mariano and Finn, Chelsea},
    url = {https://sites.google.com/view/mo2o/},
    keywords = {Model-based reinforcement learning, high-dimensional observations, offline-to-online fine-tuning}
}

@article{Sun2023Multi-compartmentLearning,
    title = {{Multi-compartment Neuron and Population Encoding improved Spiking Neural Network for Deep Distributional Reinforcement Learning}},
    year = {2023},
    author = {Sun, Yinqian and Zeng, Yi and Zhao, Feifei and Zhao, Zhuoya},
    arxivId = {2301.07275v1}
}

@article{Zhang2022Multi-SacleLearning,
    title = {{Multi-Sacle Dynamic Coding Improved Spiking Actor Network for Reinforcement Learning}},
    year = {2022},
    journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
    author = {Zhang, Duzhen and Zhang, Tielin and Jia, Shuncheng and Xu, Bo},
    number = {1},
    month = {6},
    pages = {59--67},
    volume = {36},
    publisher = {Association for the Advancement of Artificial Intelligence},
    url = {https://ojs.aaai.org/index.php/AAAI/article/view/19879},
    isbn = {1577358767},
    doi = {10.1609/AAAI.V36I1.19879},
    issn = {2374-3468},
    keywords = {Cognitive Modeling {\&} Cognitive Systems (CMS)}
}

@article{Vassiliades2011MultiagentDilemma,
    title = {{Multiagent reinforcement learning: Spiking and nonspiking agents in the Iterated Prisoner's Dilemma}},
    year = {2011},
    journal = {IEEE Transactions on Neural Networks},
    author = {Vassiliades, Vassilis and Cleanthous, Aristodemos and Christodoulou, Chris},
    number = {4},
    month = {4},
    pages = {639--653},
    volume = {22},
    doi = {10.1109/TNN.2011.2111384},
    issn = {10459227},
    pmid = {21421435},
    keywords = {Multiagent reinforcement learning, Prisoner's Dilemma, reward transformation, spiking neural networks}
}

@inproceedings{Mitchell2017NeoN:Navigation,
    title = {{NeoN: Neuromorphic control for autonomous robotic navigation}},
    shorttitle = {NeoN},
    year = {2017},
    booktitle = {2017 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS)},
    author = {Mitchell, J Parker and Bruer, Grant and Dean, Mark E and Plank, James S and Rose, Garrett S and Schuman, Catherine D},
    pages = {136--142},
    publisher = {IEEE},
    url = {http://ieeexplore.ieee.org/document/8250111/ https://www.osti.gov/servlets/purl/1423018},
    isbn = {978-1-5386-1342-9},
    doi = {10.1109/IRIS.2017.8250111},
    language = {en}
}

@article{Maass1997NetworksModels,
    title = {{Networks of spiking neurons: The third generation of neural network models}},
    year = {1997},
    journal = {Neural Networks},
    author = {Maass, Wolfgang},
    number = {9},
    month = {12},
    pages = {1659--1671},
    volume = {10},
    publisher = {Elsevier Science Ltd},
    doi = {10.1016/S0893-6080(97)00011-7},
    issn = {08936080},
    keywords = {Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron}
}

@article{Lansdell2023NeuralLearning,
    title = {{Neural spiking for causal inference and learning}},
    year = {2023},
    journal = {PLoS computational biology},
    author = {Lansdell, Benjamin James and Kording, Konrad Paul},
    number = {4},
    month = {4},
    volume = {19},
    publisher = {PLoS Comput Biol},
    url = {https://pubmed.ncbi.nlm.nih.gov/37014913/},
    doi = {10.1371/JOURNAL.PCBI.1011005},
    issn = {1553-7358},
    pmid = {37014913},
    keywords = {Action Potentials / physiology, Benjamin James Lansdell, Extramural, Konrad Paul Kording, Learning* / physiology, MEDLINE, Membrane Potentials / physiology, Models, N.I.H., NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, Neurological, Neurons* / physiology, PMC10104331, PubMed Abstract, Research Support, doi:10.1371/journal.pcbi.1011005, pmid:37014913}
}

@article{Fremaux2015NeuromodulatedRules,
    title = {{Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules}},
    year = {2015},
    journal = {Frontiers in Neural Circuits},
    author = {Fr{\'{e}}maux, Nicolas and Gerstner, Wulfram},
    number = {JAN2016},
    month = {1},
    pages = {155830},
    volume = {9},
    publisher = {Frontiers Media S.A.},
    doi = {10.3389/FNCIR.2015.00085/BIBTEX},
    issn = {16625110},
    pmid = {26834568},
    keywords = {Neuromodulation, Novelty, Plasticity, Reward learning, STDP, Spiking neuron networks, Synaptic plasticity (LTP/LTD)}
}

@article{Stroobants2022NeuromorphicQuadrotors,
    title = {{Neuromorphic computing for attitude estimation onboard quadrotors}},
    year = {2022},
    journal = {Neuromorphic Computing and Engineering},
    author = {Stroobants, Stein and Dupeyroux, Julien and de Croon, Guido C H E},
    number = {3},
    pages = {034005},
    volume = {2},
    url = {http://arxiv.org/abs/2304.08802 https://arxiv.org/pdf/2304.08802.pdf https://arxiv.org/abs/2304.08802},
    doi = {10.1088/2634-4386/ac7ee0},
    issn = {2634-4386},
    keywords = {Computer Science - Robotics}
}

@inproceedings{Patton2021NeuromorphicRacing,
    title = {{Neuromorphic Computing for Autonomous Racing}},
    year = {2021},
    booktitle = {ICONS 2021: International Conference on Neuromorphic Systems 2021},
    author = {Patton, Robert and Schuman, Catherine and Kulkarni, Shruti and Parsa, Maryam and Mitchell, J Parker and Haas, N Quentin and Stahl, Christopher and Paulissen, Spencer and Date, Prasanna and Potok, Thomas and Snyder, Shay},
    pages = {1--5},
    publisher = {ACM},
    url = {https://dl.acm.org/doi/10.1145/3477145.3477170},
    isbn = {978-1-4503-8691-3},
    doi = {10.1145/3477145.3477170},
    language = {en}
}

@article{PattonNeuromorphicAutonomous,
    title = {{Neuromorphic Computing for Autonomous Racing Computing for Autonomous}},
    author = {Patton, Robert M and Schuman, Catherine D and Kulkarni, Shruti R and Parsa, Maryam and Parker Mitchell, J and Quentin Haas, N and Stahl, Christopher and Paulissen, Spencer and Date, Prasanna and Potok, Thomas E and Snyder, Shay},
    publisher = {ACM},
    url = {https://doi.org/10.1145/3477145.3477170},
    isbn = {9781450386913},
    doi = {10.1145/3477145.3477170},
    keywords = {autonomous driving, evolutionary algo-rithms, neuromorphic computing}
}

@inproceedings{Dupeyroux2021NeuromorphicProcessor,
    title = {{Neuromorphic control for optic-flow-based landings of MAVs using the Loihi processor}},
    year = {2021},
    author = {Dupeyroux, Julien and Hagenaars, Jesse and Paredes-Vall{\'{e}}s, Federico and de Croon, Guido},
    pages = {96--102},
    url = {http://arxiv.org/abs/2011.00534 https://arxiv.org/pdf/2011.00534.pdf https://arxiv.org/abs/2011.00534},
    doi = {10.1109/ICRA48506.2021.9560937},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics}
}

@inproceedings{Dupeyroux2021NeuromorphicProcessorb,
    title = {{Neuromorphic control for optic-flow-based landings of MAVs using the Loihi processor}},
    year = {2021},
    author = {Dupeyroux, Julien and Hagenaars, Jesse and Paredes-Vall{\'{e}}s, Federico and de Croon, Guido},
    pages = {96--102},
    url = {http://arxiv.org/abs/2011.00534 https://arxiv.org/pdf/2011.00534.pdf https://arxiv.org/abs/2011.00534},
    doi = {10.1109/ICRA48506.2021.9560937},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics}
}

@article{Mead1990NeuromorphicSystems,
    title = {{Neuromorphic Electronic Systems}},
    year = {1990},
    journal = {Proceedings of the IEEE},
    author = {Mead, Carver},
    number = {10},
    pages = {1629--1636},
    volume = {78},
    doi = {10.1109/5.58356},
    issn = {15582256}
}

@article{Chen2024NoisyExploration,
    title = {{Noisy Spiking Actor Network for Exploration}},
    year = {2024},
    author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
    month = {3},
    arxivId = {2403.04162}
}

@article{Chen2024NoisyExplorationb,
    title = {{Noisy Spiking Actor Network for Exploration}},
    year = {2024},
    author = {Chen, Ding and Peng, Peixi and Huang, Tiejun and Tian, Yonghong},
    month = {3},
    arxivId = {2403.04162}
}

@article{MaassNoisyNeurons,
    title = {{Noisy Spiking Neurons with Temporal Coding have more Computational Power than Sigmoidal Neurons}},
    author = {Maass, Wolfgang}
}

@article{ZhaoODE-basedPOMDPs,
    title = {{ODE-based Recurrent Model-free Reinforcement Learning for POMDPs}},
    author = {Zhao, Xuanle and Zhang, Duzhen and Han, Liyuan and Zhang, Tielin and Xu, Bo},
    arxivId = {2309.14078v2}
}

@article{Hammami2022On-PolicyNetwork,
    title = {{On-Policy vs. Off-Policy Deep Reinforcement Learning for Resource Allocation in Open Radio Access Network}},
    year = {2022},
    journal = {IEEE Wireless Communications and Networking Conference, WCNC},
    author = {Hammami, Nessrine and Nguyen, Kim Khoa},
    pages = {1461--1466},
    volume = {2022-April},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    isbn = {9781665442664},
    doi = {10.1109/WCNC51071.2022.9771605},
    issn = {15253511},
    keywords = {5G, ACER, O-RAN, PPO, Resource allocation}
}

@article{Wurman2022OutracingLearning,
    title = {{Outracing champion Gran Turismo drivers with deep reinforcement learning}},
    year = {2022},
    journal = {Nature},
    author = {Wurman, Peter R and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and Gilpin, Leilani and Khandelwal, Piyush and Kompella, Varun and Lin, HaoChih and MacAlpine, Patrick and Oller, Declan and Seno, Takuma and Sherstan, Craig and Thomure, Michael D and Aghabozorgi, Houmehr and Barrett, Leon and Douglas, Rory and Whitehead, Dion and D{\"{u}}rr, Peter and Stone, Peter and Spranger, Michael and Kitano, Hiroaki},
    number = {7896},
    pages = {223--228},
    volume = {602},
    url = {https://www.nature.com/articles/s41586-021-04357-7 https://www.nature.com/articles/s41586-021-04357-7.pdf},
    doi = {10.1038/s41586-021-04357-7},
    issn = {1476-4687},
    keywords = {Applied mathematics, Computer science},
    language = {en}
}

@article{Zhang2021Population-codingLearning,
    title = {{Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning}},
    year = {2021},
    author = {Zhang, Duzhen and Zhang, Tielin and Jia, Shuncheng and Cheng, Xiang and Xu, Bo},
    month = {6},
    arxivId = {2106.07854}
}

@article{Akl2021PortingLoihi,
    title = {{Porting Deep Spiking Q-Networks to neuromorphic chip Loihi}},
    year = {2021},
    journal = {ACM International Conference Proceeding Series},
    author = {Akl, Mahmoud and Sandamirskaya, Yulia and Walter, Florian and Knoll, Alois},
    month = {7},
    publisher = {Association for Computing Machinery},
    url = {https://dl.acm.org/doi/10.1145/3477145.3477159},
    isbn = {9781450386913},
    doi = {10.1145/3477145.3477159},
    keywords = {Spiking neural networks, neuromorphic hardware, reinforcement learning}
}

@article{SchulmanProximalAlgorithms,
    title = {{Proximal Policy Optimization Algorithms}},
    author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Openai, Oleg Klimov},
    arxivId = {1707.06347v2}
}

@article{Kim2022RATENETWORKS,
    title = {{RATE CODING OR DIRECT CODING: WHICH ONE IS BETTER FOR ACCURATE, ROBUST, AND ENERGY-EFFICIENT SPIKING NEURAL NETWORKS?}},
    year = {2022},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    author = {Kim, Youngeun and Park, Hyoungseob and Moitra, Abhishek and Bhattacharjee, Abhiroop and Venkatesha, Yeshwanth and Panda, Priyadarshini},
    pages = {71--75},
    volume = {2022-May},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    isbn = {9781665405409},
    doi = {10.1109/ICASSP43922.2022.9747906},
    issn = {15206149},
    arxivId = {2202.03133},
    keywords = {Spiking neural network, adversarial robustness, direct coding, energy-efficiency, rate coding}
}

@article{Kapturowski2019RECURRENTLEARNING,
    title = {{RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING}},
    year = {2019},
    author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
    url = {https://openreview.net/pdf?id=r1lyTjAqYX},
    language = {en}
}

@inproceedings{StevenKapturowski2019RECURRENTLEARNING,
    title = {{RECURRENT EXPERIENCE REPLAY IN DISTRIBUTED REINFORCEMENT LEARNING}},
    year = {2019},
    booktitle = {ICLR},
    author = {{Steven Kapturowski} and {Georg Ostrovski} and {John Quan} and {Remi Munos} and {Will Dabney}}
}

@article{Ni2021RecurrentPOMDPs,
    title = {{Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs}},
    year = {2021},
    journal = {Proceedings of Machine Learning Research},
    author = {Ni, Tianwei and Eysenbach, Benjamin and Salakhutdinov, Ruslan},
    month = {10},
    pages = {16691--16723},
    volume = {162},
    publisher = {ML Research Press},
    url = {https://arxiv.org/abs/2110.05038v3},
    issn = {26403498},
    arxivId = {2110.05038}
}

@article{Yang2021RecurrentControl,
    title = {{Recurrent Off-policy Baselines for Memory-based Continuous Control}},
    year = {2021},
    author = {Yang, Zhihan and Nguyen, Hai},
    month = {10},
    url = {https://arxiv.org/abs/2110.12628v1},
    arxivId = {2110.12628}
}

@article{TangReinforcementHardware,
    title = {{Reinforcement co-Learning of Deep and Spiking Neural Networks for Energy-Efficient Mapless Navigation with Neuromorphic Hardware}},
    author = {Tang, Guangzhi and Kumar, Neelesh and Michmizos, Konstantinos P},
    url = {https://github.com/combra-lab/spiking-ddpg-mapless-navigation},
    arxivId = {2003.01157v2}
}

@article{Yuan2019ReinforcementSynapses,
    title = {{Reinforcement Learning in Spiking Neural Networks with Stochastic and Deterministic Synapses}},
    year = {2019},
    journal = {Neural computation},
    author = {Yuan, Mengwen and Wu, Xi and Yan, Rui and Tang, Huajin},
    number = {12},
    month = {12},
    pages = {2368--2389},
    volume = {31},
    publisher = {Neural Comput},
    url = {https://pubmed.ncbi.nlm.nih.gov/31614099/},
    doi = {10.1162/NECO{\_}A{\_}01238},
    issn = {1530-888X},
    pmid = {31614099},
    keywords = {Action Potentials / physiology*, Computer Simulation, Computer*, Huajin Tang, MEDLINE, Mengwen Yuan, Models, NCBI, NIH, NLM, National Center for Biotechnology Information, National Institutes of Health, National Library of Medicine, Neural Networks, Neurological*, Neuronal Plasticity / physiology*, Neurons / physiology, Non-U.S. Gov't, Psychology*, PubMed Abstract, Reinforcement, Research Support, Synapses / physiology*, Synaptic Transmission / physiology, Xi Wu, doi:10.1162/neco{\_}a{\_}01238, pmid:31614099}
}

@article{deQueiroz2006ReinforcementModel,
    title = {{Reinforcement learning of a simple control task using the spike response model}},
    year = {2006},
    journal = {Neurocomputing},
    author = {de Queiroz, Murilo Saraiva and de Berr{\^{e}}do, Roberto Coelho and de P{\'{a}}dua Braga, Antônio},
    number = {1},
    pages = {14--20},
    series = {Neural Networks},
    volume = {70},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231206002001 https://www.sciencedirect.com/science/article/pii/S0925231206002001/pdfft?md5=4eda56351b20d6053ff9445cf55966ef&pid=1-s2.0-S0925231206002001-main.pdf&isDTMRedir=Y https://www.sciencedirect.com/science/article/pii/S0925231206002001},
    doi = {10.1016/j.neucom.2006.07.002},
    issn = {0925-2312},
    keywords = {Reinforcement learning, Spike response model, Spiking neuron},
    language = {en}
}

@article{SuttonReinforcementProgress,
    title = {{Reinforcement Learning: An Introduction Second edition, in progress}},
    author = {Sutton, Richard S and Barto, Andrew G}
}

@misc{Fedus2020RevisitingReplay,
    title = {{Revisiting Fundamentals of Experience Replay}},
    year = {2020},
    author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
    publisher = {arXiv},
    url = {https://arxiv.org/pdf/2007.06700.pdf https://arxiv.org/pdf/2007.06700.pdf https://arxiv.org/abs/2007.06700},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{Neuman2021RobomorphicMorphology,
    title = {{Robomorphic computing: a design methodology for domain-specific accelerators parameterized by robot morphology}},
    shorttitle = {Robomorphic computing},
    year = {2021},
    booktitle = {ASPLOS '21: 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
    author = {Neuman, Sabrina M and Plancher, Brian and Bourgeat, Thomas and Tambe, Thierry and Devadas, Srinivas and Reddi, Vijay Janapa},
    pages = {674--686},
    publisher = {ACM},
    url = {https://dl.acm.org/doi/10.1145/3445814.3446746 https://dl.acm.org/doi/pdf/10.1145/3445814.3446746},
    isbn = {978-1-4503-8317-2},
    doi = {10.1145/3445814.3446746},
    language = {en}
}

@article{DeepmindSAMPLEREPLAY,
    title = {{SAMPLE EFFICIENT ACTOR-CRITIC WITH EXPERIENCE REPLAY}},
    author = {Deepmind, Ziyu Wang and Deepmind, Victor Bapst and Deepmind, Nicolas Heess and Deepmind, Volodymyr Mnih and Deepmind, Remi Munos and Deepmind, Koray Kavukcuoglu and De Freitas, Nando},
    arxivId = {1611.01224v2}
}

@misc{Tan2018Sim-to-Real:Robots,
    title = {{Sim-to-Real: Learning Agile Locomotion For Quadruped Robots}},
    shorttitle = {Sim-to-Real},
    year = {2018},
    author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
    publisher = {arXiv},
    url = {http://arxiv.org/abs/1804.10332 https://arxiv.org/pdf/1804.10332.pdf https://arxiv.org/abs/1804.10332},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics}
}

@misc{Speck,
    title = {{Speck}},
    howpublished = {https://www.synsense.ai/products/speck/}
}

@article{Schmidgall2021SpikePropamine:Networks,
    title = {{SpikePropamine: Differentiable Plasticity in Spiking Neural Networks}},
    year = {2021},
    journal = {Frontiers in Neurorobotics},
    author = {Schmidgall, Samuel and Ashkanazy, Julia and Lawson, Wallace and Hays, Joe},
    month = {9},
    pages = {629210},
    volume = {15},
    publisher = {Frontiers Media S.A.},
    doi = {10.3389/FNBOT.2021.629210/BIBTEX},
    issn = {16625218},
    arxivId = {2106.02681},
    keywords = {backpropagation, motor learning, neuromodulation, plasticity, reinforcement learning, robotic learning, spiking neural network, temporal learning}
}

@article{Cao2015SpikingRecognition,
    title = {{Spiking deep convolutional neural networks for energy-efficient object recognition}},
    year = {2015},
    journal = {SpringerY Cao, Y Chen, D KhoslaInternational Journal of Computer Vision, 2015•Springer},
    author = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak and by Marc, Communicated and Ranzato, Aurelio and Hinton, Geoffrey E and LeCun Cao, Yann Y and Chen, Y and Khosla, D},
    number = {1},
    month = {5},
    pages = {54--66},
    volume = {113},
    publisher = {Kluwer Academic Publishers},
    url = {https://link.springer.com/article/10.1007/s11263-014-0788-3},
    doi = {10.1007/s11263-014-0788-3},
    keywords = {Convolutional neural networks, Deep learning, Machine learning, Neuromorphic circuits, Object recognition, Spiking neural networks}
}

@article{Liu2023SpikingControl,
    title = {{Spiking Neural-Networks-Based Data-Driven Control}},
    year = {2023},
    journal = {Electronics},
    author = {Liu, Yuxiang and Pan, Wei},
    number = {2},
    pages = {310},
    volume = {12},
    url = {https://www.mdpi.com/2079-9292/12/2/310 https://www.mdpi.com/2079-9292/12/2/310/pdf?version=1673945908},
    doi = {10.3390/electronics12020310},
    issn = {2079-9292},
    keywords = {Value function with SNN, control, reinforcement learning, spiking neural network},
    language = {en}
}

@misc{Mayr2019SpiNNakerLearning,
    title = {{SpiNNaker 2: A 10 Million Core Processor System for Brain Simulation and Machine Learning}},
    year = {2019},
    author = {Mayr, Christian and Hoeppner, Sebastian and Furber, Steve},
    arxivId = {https://doi.org/10.48550/arXiv.1911.02385}
}

@article{Tan2021StrategyNetworks,
    title = {{Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks}},
    year = {2021},
    author = {Tan, Weihao and Patel, Devdhar and Kozma, Robert},
    url = {www.aaai.org}
}

@article{LuStructuredLearning,
    title = {{Structured State Space Models for In-Context Reinforcement Learning}},
    author = {Lu, Chris and Schroecker, Yannick and Gu, Albert and Parisotto, Emilio and Foerster, Jakob and Singh, Satinder and Feryal, Deepmind and Deepmind, Behbahani},
    url = {https://github.com/luchris429/s5rl.}
}

@article{Bellec2019SupplementaryNeurons,
    title = {{Supplementary materials for: A solution to the learning dilemma for recurrent networks of spiking neurons}},
    year = {2019},
    author = {Bellec, G and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, R and Maass, W}
}

@article{NeftciSurrogateNetworks,
    title = {{Surrogate Gradient Learning in Spiking Neural Networks}},
    author = {Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
    isbn = {1901.09948v2},
    arxivId = {1901.09948v2}
}

@article{SchmidgallSYNAPTICNETWORKS,
    title = {{SYNAPTIC MOTOR ADAPTATION: A THREE-FACTOR LEARNING RULE FOR ADAPTIVE ROBOTIC CONTROL IN SPIKING NEURAL NETWORKS}},
    author = {Schmidgall, Samuel and Hays, Joe},
    isbn = {2306.01906v1},
    arxivId = {2306.01906v1},
    keywords = {neuromodulation, online learning, robot learning, spiking neural network, synaptic plasticity}
}

@article{Comsa2020TemporalFunction,
    title = {{Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function}},
    year = {2020},
    journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
    author = {Comsa, Iulia M. and Fischbacher, Thomas and Potempa, Krzysztof and Gesmundo, Andrea and Versari, Luca and Alakuijala, Jyrki},
    month = {5},
    pages = {8529--8533},
    volume = {2020-May},
    publisher = {Institute of Electrical and Electronics Engineers Inc.},
    isbn = {9781509066315},
    doi = {10.1109/ICASSP40776.2020.9053856},
    issn = {15206149},
    keywords = {back-propagation, image classification, machine learning, spiking networks, temporal coding}
}

@article{Hebb2005TheTheory,
    title = {{The Organization of Behavior : A Neuropsychological Theory}},
    year = {2005},
    journal = {The Organization of Behavior},
    author = {Hebb, D.O.},
    month = {4},
    publisher = {Psychology Press},
    url = {https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb},
    isbn = {9781410612403},
    doi = {10.4324/9781410612403}
}

@incollection{Chowdhury2022TowardsPruning,
    title = {{Towards Ultra Low Latency Spiking Neural Networks for Vision and Sequential Tasks Using Temporal Pruning}},
    year = {2022},
    booktitle = {Computer Vision – ECCV 2022},
    author = {Chowdhury, Sayeed Shafayet and Rathi, Nitin and Roy, Kaushik},
    editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'{e}}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
    pages = {709--726},
    volume = {13671},
    publisher = {Springer Nature Switzerland},
    url = {https://link.springer.com/10.1007/978-3-031-20083-0_42 https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710709.pdf},
    address = {Cham},
    isbn = {978-3-031-20082-3 978-3-031-20083-0},
    language = {en}
}

@article{Eshraghian2023TRAININGLEARNING,
    title = {{TRAINING SPIKING NEURAL NETWORKS USING LESSONS FROM DEEP LEARNING}},
    year = {2023},
    journal = {TRAINING SPIKING NEURAL NETWORKS USING LESSONS FROM DEEP LEARNING},
    author = {Eshraghian, Jason K and Ward, Max and Forschungszentrum, Emre Neftci and Rwth, Jülich and Wang, Aachen Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D},
    url = {https://snntorch.readthedocs.io/en/latest/tutorials/index.html.},
    arxivId = {2109.12894v6}
}

@article{Wayne2018UnsupervisedAgent,
    title = {{Unsupervised Predictive Memory in a Goal-Directed Agent}},
    year = {2018},
    author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and Grabska-Barwi´nskabarwi´nska, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
    arxivId = {1803.10760v1}
}

@article{ViewpointTransmission,
    title = {{Viewpoint Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission}}
}

@article{ViewpointTransmissionb,
    title = {{Viewpoint Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission}}
}

@article{Ha2018WorldModels,
    title = {{World Models}},
    year = {2018},
    author = {Ha, David and Schmidhuber, Jürgen},
    url = {http://arxiv.org/abs/1803.10122 https://arxiv.org/pdf/1803.10122.pdf https://arxiv.org/abs/1803.10122},
    doi = {10.5281/zenodo.1207631},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@inproceedings{brisset2006Paparazzi,
  TITLE = {{The Paparazzi Solution}},
  AUTHOR = {Brisset, Pascal and Drouin, Antoine and Gorraz, Michel and Huard, Pierre-Selim and Tyler, Jeremy},
  URL = {https://enac.hal.science/hal-01004157},
  BOOKTITLE = {{MAV 2006, 2nd US-European Competition and Workshop on Micro Air Vehicles}},
  ADDRESS = {Sandestin, United States},
  PAGES = {pp xxxx},
  YEAR = {2006},
  MONTH = Oct,
  PDF = {https://enac.hal.science/hal-01004157/file/Brisset_MAV2006.pdf},
  HAL_ID = {hal-01004157},
  HAL_VERSION = {v1},
}

@article{Mei2021ZigZag:Accelerators,
    title = {{ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators}},
    shorttitle = {ZigZag},
    year = {2021},
    journal = {IEEE Transactions on Computers},
    author = {Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
    number = {8},
    pages = {1160--1174},
    volume = {70},
    url = {https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9360462&ref= https://ieeexplore.ieee.org/abstract/document/9360462},
    doi = {10.1109/TC.2021.3059962},
    issn = {1557-9956},
    keywords = {Analytical models, Computer architecture, DNN, Hardware, Neural networks, Search engines, Search problems, Space exploration, accelerator, analytical model, dataflow, design space exploration, mapping, memory hierarchy, scheduling}
@ARTICLE{STDP_survey,
  
AUTHOR={Markram, Henry and Gerstner, Wulfram and Sjöström, Per Jesper},   
	 
TITLE={Spike-Timing-Dependent Plasticity: A Comprehensive Overview},      
	
JOURNAL={Frontiers in Synaptic Neuroscience},      
	
VOLUME={4},           
	
YEAR={2012},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnsyn.2012.00002},       
	
DOI={10.3389/fnsyn.2012.00002},      
	
ISSN={1663-3563}   
   
}
@inproceedings{GRANT,
	address = {Glasgow, United Kingdom},
	title = {{GRANT}: {Ground}-{Roaming} {Autonomous} {Neuromorphic} {Targeter}},
	isbn = {978-1-72816-926-2},
	shorttitle = {{GRANT}},
	url = {https://ieeexplore.ieee.org/document/9207276/},
	doi = {10.1109/IJCNN48605.2020.9207276},
	abstract = {In this work we describe the design, implementation, and testing of the ﬁrst neuromorphic robot capable of obstacle avoidance, grid coverage, and targeting controlled by the second generation Dynamic Adaptive Neural Network Array (DANNA2) digital spiking neuromorphic processor. The simplicity of the DANNA2 processor along with the TENNLab hardware/software co-design framework allows for compact spiking networks that can run efﬁciently on a small, resource-constrained, platform such as a Xilinx Artix-7 ﬁeld-programmable gate array. Additionally, we present the dynamic reconﬁgurability of DANNA2 arrays as a method of realizing complex, multi-objective tasks on hardware that is restricted to relatively small networks.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Ambrose, Jonathan D. and Foshie, Adam Z. and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Mitchell, J. Parker and Schuman, Catherine D. and Bruer, Grant},
	month = jul,
	year = {2020},
	pages = {1--8},
}

@inproceedings{NeuromorphicRacer,
	address = {Knoxville TN USA},
	title = {Neuromorphic {Computing} for {Autonomous} {Racing}},
	isbn = {978-1-4503-8691-3},
	url = {https://dl.acm.org/doi/10.1145/3477145.3477170},
	doi = {10.1145/3477145.3477170},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {International {Conference} on {Neuromorphic} {Systems} 2021},
	publisher = {ACM},
	author = {Patton, Robert and Schuman, Catherine and Kulkarni, Shruti and Parsa, Maryam and Mitchell, J. Parker and Haas, N. Quentin and Stahl, Christopher and Paulissen, Spencer and Date, Prasanna and Potok, Thomas and Snyder, Shay},
	month = jul,
	year = {2021},
	pages = {1--5},
}

@misc{TUDelftNeuromorphic,
	title = {Fully neuromorphic vision and control for autonomous drone flight},
	url = {http://arxiv.org/abs/2303.08778},
	abstract = {Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions due to the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present the first fully neuromorphic vision-to-control pipeline for controlling a freely flying drone. Specifically, we train a spiking neural network that accepts high-dimensional raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28.8k neurons, maps incoming raw events to ego-motion estimates and is trained with self-supervised learning on real event data. The control part consists of a single decoding layer and is learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone can accurately follow different ego-motion setpoints, allowing for hovering, landing, and maneuvering sideways\${\textbackslash}unicode\{x2014\}\$even while yawing at the same time. The neuromorphic pipeline runs on board on Intel's Loihi neuromorphic processor with an execution frequency of 200 Hz, spending only 27 \${\textbackslash}unicode\{x00b5\}\$J per inference. These results illustrate the potential of neuromorphic sensing and processing for enabling smaller, more intelligent robots.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Paredes-Vallés, Federico and Hagenaars, Jesse and Dupeyroux, Julien and Stroobants, Stein and Xu, Yingfu and de Croon, Guido},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08778 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@article{stroobants_neuromorphic_2022,
	title = {Neuromorphic computing for attitude estimation onboard quadrotors},
	volume = {2},
	issn = {2634-4386},
	url = {http://arxiv.org/abs/2304.08802},
	doi = {10.1088/2634-4386/ac7ee0},
	abstract = {Compelling evidence has been given for the high energy efficiency and update rates of neuromorphic processors, with performance beyond what standard Von Neumann architectures can achieve. Such promising features could be advantageous in critical embedded systems, especially in robotics. To date, the constraints inherent in robots (e.g., size and weight, battery autonomy, available sensors, computing resources, processing time, etc.), and particularly in aerial vehicles, severely hamper the performance of fully-autonomous on-board control, including sensor processing and state estimation. In this work, we propose a spiking neural network (SNN) capable of estimating the pitch and roll angles of a quadrotor in highly dynamic movements from 6-degree of freedom Inertial Measurement Unit (IMU) data. With only 150 neurons and a limited training dataset obtained using a quadrotor in a real world setup, the network shows competitive results as compared to state-of-the-art, non-neuromorphic attitude estimators. The proposed architecture was successfully tested on the Loihi neuromorphic processor on-board a quadrotor to estimate the attitude when flying. Our results show the robustness of neuromorphic attitude estimation and pave the way towards energy-efficient, fully autonomous control of quadrotors with dedicated neuromorphic computing systems.},
	number = {3},
	urldate = {2023-08-07},
	journal = {Neuromorphic Computing and Engineering},
	author = {Stroobants, Stein and Dupeyroux, Julien and de Croon, Guido C. H. E.},
	month = sep,
	year = {2022},
	note = {arXiv:2304.08802 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {034005},
}
@article{neftci2019surrogate,
  title={Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks},
  author={Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine},
  volume={36},
  number={6},
  pages={51--63},
  year={2019},
  publisher={IEEE}
}
@inproceedings{dupeyroux_neuromorphic_2021,
	title = {Neuromorphic control for optic-flow-based landings of {MAVs} using the {Loihi} processor},
	url = {http://arxiv.org/abs/2011.00534},
	doi = {10.1109/ICRA48506.2021.9560937},
	abstract = {Neuromorphic processors like Loihi offer a promising alternative to conventional computing modules for endowing constrained systems like micro air vehicles (MAVs) with robust, efficient and autonomous skills such as take-off and landing, obstacle avoidance, and pursuit. However, a major challenge for using such processors on robotic platforms is the reality gap between simulation and the real world. In this study, we present for the very first time a fully embedded application of the Loihi neuromorphic chip prototype in a flying robot. A spiking neural network (SNN) was evolved to compute the thrust command based on the divergence of the ventral optic flow field to perform autonomous landing. Evolution was performed in a Python-based simulator using the PySNN library. The resulting network architecture consists of only 35 neurons distributed among 3 layers. Quantitative analysis between simulation and Loihi reveals a root-mean-square error of the thrust setpoint as low as 0.005 g, along with a 99.8\% matching of the spike sequences in the hidden layer, and 99.7\% in the output layer. The proposed approach successfully bridges the reality gap, offering important insights for future neuromorphic applications in robotics. Supplementary material is available at https://mavlab.tudelft.nl/loihi/.},
	urldate = {2023-08-07},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Dupeyroux, Julien and Hagenaars, Jesse and Paredes-Vallés, Federico and de Croon, Guido},
	month = may,
	year = {2021},
	note = {arXiv:2011.00534 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	pages = {96--102},
}

@inproceedings{rebecq_esim_2018,
	title = {{ESIM}: an {Open} {Event} {Camera} {Simulator}},
	shorttitle = {{ESIM}},
	url = {https://proceedings.mlr.press/v87/rebecq18a.html},
	abstract = {Event cameras are revolutionary sensors that work radically differently from standard cameras. Instead of capturing intensity images at a fixed rate, event cameras measure changes of intensity asynchronously, in the form of a stream of events, which encode per-pixel brightness changes. In the last few years, their outstanding properties (asynchronous sensing, no motion blur, high dynamic range) have led to exciting vision applications, with very low-latency and high robustness. However, these sensors are still scarce and expensive to get, slowing down progress of the research community. To address these issues, there is a huge demand for cheap, high-quality synthetic, labeled event for algorithm prototyping, deep learning and algorithm benchmarking. The development of such a simulator, however, is not trivial since event cameras work fundamentally differently from frame-based cameras. We present the first event camera simulator that can generate a large amount of reliable event data. The key component of our simulator is a theoretically sound, adaptive rendering scheme that only samples frames when necessary, through a tight coupling between the rendering engine and the event simulator. We release an open source implementation of our simulator.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Rebecq, Henri and Gehrig, Daniel and Scaramuzza, Davide},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {969--982},
}

@inproceedings{maqueda_event-based_2018,
	address = {Salt Lake City, UT},
	title = {Event-{Based} {Vision} {Meets} {Deep} {Learning} on {Steering} {Prediction} for {Self}-{Driving} {Cars}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578666/},
	doi = {10.1109/CVPR.2018.00568},
	abstract = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, ﬁltering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle’s steering angle. To make the best out of this sensor–algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (≈1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Maqueda, Ana I. and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
	month = jun,
	year = {2018},
	pages = {5419--5427},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μμs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbrück, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jörg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Brightness, Cameras, Event cameras, Retina, Robot vision systems, Voltage control, asynchronous sensor, bio-inspired vision, high dynamic range, low latency, low power},
	pages = {154--180},
}

@inproceedings{vitale_event-driven_2021,
	address = {Xi'an, China},
	title = {Event-driven {Vision} and {Control} for {UAVs} on a {Neuromorphic} {Chip}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9560881/},
	doi = {10.1109/ICRA48506.2021.9560881},
	abstract = {Event-based vision enables ultra-low latency visual event-images are usually created by accumulating events in feedback and low power consumption, which are key require- conventional matrix structures based on ﬁxed time intervals ments for high-speed control of unmanned aerial vehicles.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Vitale, Antonio and Renner, Alpha and Nauer, Celine and Scaramuzza, Davide and Sandamirskaya, Yulia},
	month = may,
	year = {2021},
	pages = {103--109},
}

@misc{tan_sim--real_2018,
	title = {Sim-to-{Real}: {Learning} {Agile} {Locomotion} {For} {Quadruped} {Robots}},
	shorttitle = {Sim-to-{Real}},
	url = {http://arxiv.org/abs/1804.10332},
	abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
	month = may,
	year = {2018},
	note = {arXiv:1804.10332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{mitchell_neon_2017,
	address = {Ottawa, ON},
	title = {{NeoN}: {Neuromorphic} control for autonomous robotic navigation},
	isbn = {978-1-5386-1342-9},
	shorttitle = {{NeoN}},
	url = {http://ieeexplore.ieee.org/document/8250111/},
	doi = {10.1109/IRIS.2017.8250111},
	abstract = {In this paper we describe the use of a new neuromorphic computing framework to implement the navigation system for a roaming, obstacle avoidance robot. Using a Dynamic Adaptive Neural Network Array (DANNA) structure, our TENNLab (Laboratory of Tennesseans Exploring Neural Networks) hardware/software co-design framework and evolutionary optimization (EO) as the training algorithm, we create, train, implement, and test a spiking neural network autonomous robot control system using an array of neuromorphic computing elements built on an FPGA. The simplicity and ﬂexibility of the DANNA neuromorphic computing elements allow for sufﬁcient scale and connectivity on a Xilinx Kintex-7 FPGA to support sensory input and motor control for a mobile robot to navigate a dynamically changing environment. We further describe how more complex capabilities can be added using the same platform, e.g. object identiﬁcation and tracking.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Robotics} and {Intelligent} {Sensors} ({IRIS})},
	publisher = {IEEE},
	author = {Mitchell, J. Parker and Bruer, Grant and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Schuman, Catherine D.},
	month = oct,
	year = {2017},
	pages = {136--142},
}

@inproceedings{mitchell_neon_2017-1,
	address = {Ottawa, ON},
	title = {{NeoN}: {Neuromorphic} control for autonomous robotic navigation},
	isbn = {978-1-5386-1342-9},
	shorttitle = {{NeoN}},
	url = {http://ieeexplore.ieee.org/document/8250111/},
	doi = {10.1109/IRIS.2017.8250111},
	abstract = {In this paper we describe the use of a new neuromorphic computing framework to implement the navigation system for a roaming, obstacle avoidance robot. Using a Dynamic Adaptive Neural Network Array (DANNA) structure, our TENNLab (Laboratory of Tennesseans Exploring Neural Networks) hardware/software co-design framework and evolutionary optimization (EO) as the training algorithm, we create, train, implement, and test a spiking neural network autonomous robot control system using an array of neuromorphic computing elements built on an FPGA. The simplicity and ﬂexibility of the DANNA neuromorphic computing elements allow for sufﬁcient scale and connectivity on a Xilinx Kintex-7 FPGA to support sensory input and motor control for a mobile robot to navigate a dynamically changing environment. We further describe how more complex capabilities can be added using the same platform, e.g. object identiﬁcation and tracking.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Robotics} and {Intelligent} {Sensors} ({IRIS})},
	publisher = {IEEE},
	author = {Mitchell, J. Parker and Bruer, Grant and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Schuman, Catherine D.},
	month = oct,
	year = {2017},
	pages = {136--142},
}

@article{rueckauer_conversion_2017,
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	urldate = {2023-07-11},
	journal = {Frontiers in Neuroscience},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	year = {2017},
	keywords = {LI neuron is basically ReLU},
}

@article{liu_spiking_2023,
	title = {Spiking {Neural}-{Networks}-{Based} {Data}-{Driven} {Control}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/310},
	doi = {10.3390/electronics12020310},
	abstract = {Machine learning can be effectively applied in control loops to make optimal control decisions robustly. There is increasing interest in using spiking neural networks (SNNs) as the apparatus for machine learning in control engineering because SNNs can potentially offer high energy efficiency, and new SNN-enabling neuromorphic hardware is being rapidly developed. A defining characteristic of control problems is that environmental reactions and delayed rewards must be considered. Although reinforcement learning (RL) provides the fundamental mechanisms to address such problems, implementing these mechanisms in SNN learning has been underexplored. Previously, spike-timing-dependent plasticity learning schemes (STDP) modulated by factors of temporal difference (TD-STDP) or reward (R-STDP) have been proposed for RL with SNN. Here, we designed and implemented an SNN controller to explore and compare these two schemes by considering cart-pole balancing as a representative example. Although the TD-based learning rules are very general, the resulting model exhibits rather slow convergence, producing noisy and imperfect results even after prolonged training. We show that by integrating the understanding of the dynamics of the environment into the reward function of R-STDP, a robust SNN-based controller can be learned much more efficiently than TD-STDP.},
	language = {en},
	number = {2},
	urldate = {2023-07-07},
	journal = {Electronics},
	author = {Liu, Yuxiang and Pan, Wei},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Value function with SNN, control, reinforcement learning, spiking neural network},
	pages = {310},
}

@article{mei_zigzag_2021,
	title = {{ZigZag}: {Enlarging} {Joint} {Architecture}-{Mapping} {Design} {Space} {Exploration} for {DNN} {Accelerators}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {{ZigZag}},
	doi = {10.1109/TC.2021.3059962},
	abstract = {Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag's uneven mapping capabilities.},
	number = {8},
	journal = {IEEE Transactions on Computers},
	author = {Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Analytical models, Computer architecture, DNN, Hardware, Neural networks, Search engines, Search problems, Space exploration, accelerator, analytical model, dataflow, design space exploration, mapping, memory hierarchy, scheduling},
	pages = {1160--1174},
}

@misc{fedus_revisiting_2020,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	url = {https://arxiv.org/pdf/2007.06700.pdf},
	abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay -- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	month = jul,
	year = {2020},
	note = {arXiv:2007.06700 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difﬁculty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ﬁrst challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
	language = {en},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv:1506.02438 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, loss function},
}

@article{scherr_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17236-y},
	doi = {10.1038/s41467-020-17236-y},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Nature Communications},
	author = {Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models, Neuroscience, Read, Synaptic plasticity},
	pages = {3625},
}

@article{de_queiroz_reinforcement_2006,
	series = {Neural {Networks}},
	title = {Reinforcement learning of a simple control task using the spike response model},
	volume = {70},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231206002001},
	doi = {10.1016/j.neucom.2006.07.002},
	abstract = {In this work, we propose a variation of a direct reinforcement learning algorithm, suitable for usage with spiking neurons based on the spike response model (SRM). The SRM is a biologically inspired, flexible model of spiking neuron based on kernel functions that describe the effect of spike reception and emission on the membrane potential of the neuron. In our experiments, the spikes emitted by a SRM neuron are used as input signals in a simple control task. The reinforcement signal obtained from the environment is used by the direct reinforcement learning algorithm, that modifies the synaptic weights of the neuron, adjusting the spiking firing times in order to obtain a better performance at the given problem. The obtained results are comparable to those from classic methods based on value function approximation and temporal difference, for simple control tasks.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Neurocomputing},
	author = {de Queiroz, Murilo Saraiva and de Berrêdo, Roberto Coelho and de Pádua Braga, Antônio},
	month = dec,
	year = {2006},
	keywords = {Reinforcement learning, Spike response model, Spiking neuron},
	pages = {14--20},
}

@article{DSQN_Atari,
	title = {Deep {Reinforcement} {Learning} with {Spiking} {Q}-learning},
	abstract = {With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of nonspiking neurons as the representation of Q-value, which can directly learn robust policies from highdimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN.},
	journal = {arXiv.org},
	author = {{Ding Chen} and {Peixi Peng} and {Tiejun Huang} and {Yonghong Tian}},
	year = {2022},
	note = {ARXIV\_ID: 2201.09754
S2ID: 0a3104f2ca2308ac9930dd57cfbbe112d04f841d},
	keywords = {read},
}

@misc{noauthor_co_nodate,
	title = {C\&{O} project - {OneDrive}},
	url = {https://tud365-my.sharepoint.com/personal/korneelvandenb_tudelft_nl//_layouts/15/onedrive.aspx?login_hint=korneelvandenb%40tudelft%2Enl&id=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project%2FRL%20of%20control%20task%20with%20spike%20response%2Epdf&parent=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project},
	urldate = {2023-06-07},
	keywords = {not read},
}

@misc{A3C,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv:1602.01783 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
}

@article{ha_world_2018,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	urldate = {2023-06-07},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10122 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{SNN_worm,
	title = {A reinforcement learning algorithm for spiking neural networks},
	doi = {10.1109/synasc.2005.13},
	abstract = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
	journal = {Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
	author = {Florian, Răzvan V.},
	month = sep,
	year = {2005},
	doi = {10.1109/synasc.2005.13},
	note = {MAG ID: 2120905747
S2ID: cb6442b823c13339446a21fcb089428ec521a34c},
	pages = {299--306},
}

@article{bellec_biologically_2019,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets.},
	volume = {2019},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	journal = {arXiv: Neural and Evolutionary Computing},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jan,
	year = {2019},
	note = {ARXIV\_ID: 1901.09049
MAG ID: 2913168413
S2ID: ffd0f679a631b733ba2f779ce1aa0e5bbde3a8b0},
	pages = {1--37},
}

@article{DQN_Atari_human-level_2022,
	title = {Human-{Level} {Control} {Through} {Directly} {Trained} {Deep} {Spiking} \${Q}\$-{Networks}},
	doi = {10.1109/tcyb.2022.3198259},
	abstract = {As the third-generation neural networks, spiking neural networks (SNNs) have great potential on neuromorphic hardware because of their high energy efficiency. However, deep spiking reinforcement learning (DSRL), that is, the reinforcement learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the nondifferentiable property of the spiking function. To address these issues, we propose a deep spiking {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DSQN) in this article. Specifically, we propose a directly trained DSRL architecture based on the leaky integrate-and-fire (LIF) neurons and deep {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DQN). Then, we adapt a direct spiking learning algorithm for the DSQN. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, generalization and energy efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly trained SNN.},
	journal = {IEEE transactions on cybernetics},
	author = {Liu, Guisong and {Wenjie Deng} and Xie, Xiurui and {Li Huang} and Tang, Huajin},
	month = jan,
	year = {2022},
	doi = {10.1109/tcyb.2022.3198259},
	pmid = {36063509},
	note = {ARXIV\_ID: 2201.07211
MAG ID: 4294691690
S2ID: 2190a17a5e937c065adc5c139b563026ac174136},
	pages = {1--12},
}

@article{DSQN_AirSim,
	title = {Artificial versus spiking neural networks for reinforcement learning in {UAV} obstacle avoidance},
	doi = {10.1145/3528416.3530865},
	abstract = {Spiking Neural Networks (SNN) are gaining more interest from the scientific community thanks to the promise of greater energy-efficient and greater computational power. This poses several challenges as today's SNN training for RL is based on Artificial Neural Network (ANN) training and then conversion from ANN to SNN, which does not leverage SNN event-based processing inherent capabilities. The present work compares an ANN and an SNN in an event-camera-based obstacle avoidance task, trained with Reinforcement Learning (RL) using the Deep Q-Learning (DQL) algorithm. We create an experimental setup composed of Unreal Engine 4, AirSim, and an event camera that simulates a real-world obstacle avoidance environment. Additionally, we train an SNN with a gradient-based training method enabling the use of all their expressiveness even in the training phase, showing comparable performance between the ANN and the SNN. To the best of our knowledge, we are the first that implements an entire realistic pipeline with a photo-realistic simulator (Airsim) and train an SNN without converting it from a pre-trained ANN.},
	journal = {ACM International Conference on Computing Frontiers},
	author = {{Luca Zanatta} and {Francesco Barchi} and {Andrea Bartolini} and {Andrea Acquaviva}},
	month = may,
	year = {2022},
	doi = {10.1145/3528416.3530865},
	note = {MAG ID: 4229040499
S2ID: 0903d078a954427d8c875da922d52d187981b958},
}

@article{seung_learning_2003,
	title = {Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.},
	volume = {40},
	doi = {10.1016/s0896-6273(03)00761-x},
	abstract = {Abstract  It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are "hedonistic," responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
	number = {6},
	journal = {Neuron},
	author = {Seung, H. Sebastian},
	month = dec,
	year = {2003},
	doi = {10.1016/s0896-6273(03)00761-x},
	pmid = {14687542},
	note = {MAG ID: 2061897041},
	pages = {1063--1073},
}
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
@article{g_bellec_supplementary_2019,
	title = {Supplementary materials for: {A} solution to the learning dilemma for recurrent networks of spiking neurons},
	abstract = {S2 Optimization and regularization procedures 4 S2.1 Optimization procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.2 Firing rate regularization for LSNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.3 Weight decay regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 S2.4 Optimization with rewiring for sparse network connectivity . . . . . . . . . . . . . . . . . . 5},
	author = {{G. Bellec} and {Franz Scherr} and {Anand Subramoney} and {Elias Hajek} and {Darjan Salaj} and {R. Legenstein} and {W. Maass}},
	year = {2019},
	note = {S2ID: b3d964aa6bba3358b6921b36a19bae6454871498},
}

@article{eProp_pong,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {2019},
	doi = {10.1101/738385},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. But in spite of extensive research, it has remained open how they can learn through synaptic plasticity to carry out complex network computations. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A new mathematical insight tells us how these pieces need to be combined to enable biologically plausible online  network learning through gradient descent, in particular deep reinforcement learning. This new learning method -- called e-prop -- approaches the performance of BPTT (backpropagation through time), the best known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in novel energy-efficient spike-based hardware for AI.},
	journal = {bioRxiv},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = aug,
	year = {2019},
	doi = {10.1101/738385},
	pmcid = {7367848},
	pmid = {32681001},
	note = {MAG ID: 2967417697
S2ID: 858549b00245aadc92f91a2540f01398f5f389ae},
	pages = {738385},
}

@article{kapturowski_recurrent_2019,
	title = {{RECURRENT} {EXPERIENCE} {REPLAY} {IN} {DISTRIBUTED} {REINFORCEMENT} {LEARNING}},
	abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and ﬁxed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the ﬁrst agent to exceed human-level performance in 52 of the 57 Atari games.},
	language = {en},
	author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
	year = {2019},
}

@misc{bellec_biologically_2019-1,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets},
	url = {http://arxiv.org/abs/1901.09049},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = feb,
	year = {2019},
	note = {arXiv:1901.09049 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@misc{A2CvsA3C, 
url={https://openai.com/research/openai-baselines-acktr-a2c}, 
journal={OpenAI Baselines: ACKTR &amp; A2C}
} 

@inproceedings{neuman_robomorphic_2021,
	address = {Virtual USA},
	title = {Robomorphic computing: a design methodology for domain-specific accelerators parameterized by robot morphology},
	isbn = {978-1-4503-8317-2},
	shorttitle = {Robomorphic computing},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446746},
	doi = {10.1145/3445814.3446746},
	language = {en},
	urldate = {2023-08-08},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Neuman, Sabrina M. and Plancher, Brian and Bourgeat, Thomas and Tambe, Thierry and Devadas, Srinivas and Reddi, Vijay Janapa},
	month = apr,
	year = {2021},
	pages = {674--686},
}

@inproceedings{ambrose_grant_2020,
	address = {Glasgow, United Kingdom},
	title = {{GRANT}: {Ground}-{Roaming} {Autonomous} {Neuromorphic} {Targeter}},
	isbn = {978-1-72816-926-2},
	shorttitle = {{GRANT}},
	url = {https://ieeexplore.ieee.org/document/9207276/},
	doi = {10.1109/IJCNN48605.2020.9207276},
	abstract = {In this work we describe the design, implementation, and testing of the ﬁrst neuromorphic robot capable of obstacle avoidance, grid coverage, and targeting controlled by the second generation Dynamic Adaptive Neural Network Array (DANNA2) digital spiking neuromorphic processor. The simplicity of the DANNA2 processor along with the TENNLab hardware/software co-design framework allows for compact spiking networks that can run efﬁciently on a small, resource-constrained, platform such as a Xilinx Artix-7 ﬁeld-programmable gate array. Additionally, we present the dynamic reconﬁgurability of DANNA2 arrays as a method of realizing complex, multi-objective tasks on hardware that is restricted to relatively small networks.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Ambrose, Jonathan D. and Foshie, Adam Z. and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Mitchell, J. Parker and Schuman, Catherine D. and Bruer, Grant},
	month = jul,
	year = {2020},
	pages = {1--8},
}

@inproceedings{patton_neuromorphic_2021,
	address = {Knoxville TN USA},
	title = {Neuromorphic {Computing} for {Autonomous} {Racing}},
	isbn = {978-1-4503-8691-3},
	url = {https://dl.acm.org/doi/10.1145/3477145.3477170},
	doi = {10.1145/3477145.3477170},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {International {Conference} on {Neuromorphic} {Systems} 2021},
	publisher = {ACM},
	author = {Patton, Robert and Schuman, Catherine and Kulkarni, Shruti and Parsa, Maryam and Mitchell, J. Parker and Haas, N. Quentin and Stahl, Christopher and Paulissen, Spencer and Date, Prasanna and Potok, Thomas and Snyder, Shay},
	month = jul,
	year = {2021},
	pages = {1--5},
}

@inproceedings{rebecq_esim_2018,
	title = {{ESIM}: an {Open} {Event} {Camera} {Simulator}},
	shorttitle = {{ESIM}},
	url = {https://proceedings.mlr.press/v87/rebecq18a.html},
	abstract = {Event cameras are revolutionary sensors that work radically differently from standard cameras. Instead of capturing intensity images at a fixed rate, event cameras measure changes of intensity asynchronously, in the form of a stream of events, which encode per-pixel brightness changes. In the last few years, their outstanding properties (asynchronous sensing, no motion blur, high dynamic range) have led to exciting vision applications, with very low-latency and high robustness. However, these sensors are still scarce and expensive to get, slowing down progress of the research community. To address these issues, there is a huge demand for cheap, high-quality synthetic, labeled event for algorithm prototyping, deep learning and algorithm benchmarking. The development of such a simulator, however, is not trivial since event cameras work fundamentally differently from frame-based cameras. We present the first event camera simulator that can generate a large amount of reliable event data. The key component of our simulator is a theoretically sound, adaptive rendering scheme that only samples frames when necessary, through a tight coupling between the rendering engine and the event simulator. We release an open source implementation of our simulator.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Rebecq, Henri and Gehrig, Daniel and Scaramuzza, Davide},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {969--982},
}

@inproceedings{maqueda_event-based_2018,
	address = {Salt Lake City, UT},
	title = {Event-{Based} {Vision} {Meets} {Deep} {Learning} on {Steering} {Prediction} for {Self}-{Driving} {Cars}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578666/},
	doi = {10.1109/CVPR.2018.00568},
	abstract = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, ﬁltering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle’s steering angle. To make the best out of this sensor–algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (≈1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Maqueda, Ana I. and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
	month = jun,
	year = {2018},
	pages = {5419--5427},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μμs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbrück, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jörg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Brightness, Cameras, Event cameras, Retina, Robot vision systems, Voltage control, asynchronous sensor, bio-inspired vision, high dynamic range, low latency, low power},
	pages = {154--180},
}

@inproceedings{vitale_event-driven_2021,
	address = {Xi'an, China},
	title = {Event-driven {Vision} and {Control} for {UAVs} on a {Neuromorphic} {Chip}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9560881/},
	doi = {10.1109/ICRA48506.2021.9560881},
	abstract = {Event-based vision enables ultra-low latency visual event-images are usually created by accumulating events in feedback and low power consumption, which are key require- conventional matrix structures based on ﬁxed time intervals ments for high-speed control of unmanned aerial vehicles.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Vitale, Antonio and Renner, Alpha and Nauer, Celine and Scaramuzza, Davide and Sandamirskaya, Yulia},
	month = may,
	year = {2021},
	pages = {103--109},
}

@misc{tan_sim--real_2018,
	title = {Sim-to-{Real}: {Learning} {Agile} {Locomotion} {For} {Quadruped} {Robots}},
	shorttitle = {Sim-to-{Real}},
	url = {http://arxiv.org/abs/1804.10332},
	abstract = {Designing agile locomotion for quadruped robots often requires extensive expertise and tedious manual tuning. In this paper, we present a system to automate this process by leveraging deep reinforcement learning techniques. Our system can learn quadruped locomotion from scratch using simple reward signals. In addition, users can provide an open loop reference to guide the learning process when more control over the learned gait is needed. The control policies are learned in a physics simulator and then deployed on real robots. In robotics, policies trained in simulation often do not transfer to the real world. We narrow this reality gap by improving the physics simulator and learning robust policies. We improve the simulation using system identification, developing an accurate actuator model and simulating latency. We learn robust controllers by randomizing the physical environments, adding perturbations and designing a compact observation space. We evaluate our system on two agile locomotion gaits: trotting and galloping. After learning in simulation, a quadruped robot can successfully perform both gaits in the real world.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Tan, Jie and Zhang, Tingnan and Coumans, Erwin and Iscen, Atil and Bai, Yunfei and Hafner, Danijar and Bohez, Steven and Vanhoucke, Vincent},
	month = may,
	year = {2018},
	note = {arXiv:1804.10332 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{mitchell_neon_2017,
	address = {Ottawa, ON},
	title = {{NeoN}: {Neuromorphic} control for autonomous robotic navigation},
	isbn = {978-1-5386-1342-9},
	shorttitle = {{NeoN}},
	url = {http://ieeexplore.ieee.org/document/8250111/},
	doi = {10.1109/IRIS.2017.8250111},
	abstract = {In this paper we describe the use of a new neuromorphic computing framework to implement the navigation system for a roaming, obstacle avoidance robot. Using a Dynamic Adaptive Neural Network Array (DANNA) structure, our TENNLab (Laboratory of Tennesseans Exploring Neural Networks) hardware/software co-design framework and evolutionary optimization (EO) as the training algorithm, we create, train, implement, and test a spiking neural network autonomous robot control system using an array of neuromorphic computing elements built on an FPGA. The simplicity and ﬂexibility of the DANNA neuromorphic computing elements allow for sufﬁcient scale and connectivity on a Xilinx Kintex-7 FPGA to support sensory input and motor control for a mobile robot to navigate a dynamically changing environment. We further describe how more complex capabilities can be added using the same platform, e.g. object identiﬁcation and tracking.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Robotics} and {Intelligent} {Sensors} ({IRIS})},
	publisher = {IEEE},
	author = {Mitchell, J. Parker and Bruer, Grant and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Schuman, Catherine D.},
	month = oct,
	year = {2017},
	pages = {136--142},
}

@inproceedings{mitchell_neon_2017-1,
	address = {Ottawa, ON},
	title = {{NeoN}: {Neuromorphic} control for autonomous robotic navigation},
	isbn = {978-1-5386-1342-9},
	shorttitle = {{NeoN}},
	url = {http://ieeexplore.ieee.org/document/8250111/},
	doi = {10.1109/IRIS.2017.8250111},
	abstract = {In this paper we describe the use of a new neuromorphic computing framework to implement the navigation system for a roaming, obstacle avoidance robot. Using a Dynamic Adaptive Neural Network Array (DANNA) structure, our TENNLab (Laboratory of Tennesseans Exploring Neural Networks) hardware/software co-design framework and evolutionary optimization (EO) as the training algorithm, we create, train, implement, and test a spiking neural network autonomous robot control system using an array of neuromorphic computing elements built on an FPGA. The simplicity and ﬂexibility of the DANNA neuromorphic computing elements allow for sufﬁcient scale and connectivity on a Xilinx Kintex-7 FPGA to support sensory input and motor control for a mobile robot to navigate a dynamically changing environment. We further describe how more complex capabilities can be added using the same platform, e.g. object identiﬁcation and tracking.},
	language = {en},
	urldate = {2023-08-07},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Robotics} and {Intelligent} {Sensors} ({IRIS})},
	publisher = {IEEE},
	author = {Mitchell, J. Parker and Bruer, Grant and Dean, Mark E. and Plank, James S. and Rose, Garrett S. and Schuman, Catherine D.},
	month = oct,
	year = {2017},
	pages = {136--142},
}

@article{rueckauer_conversion_2017,
	title = {Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient} {Event}-{Driven} {Networks} for {Image} {Classification}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2017.00682},
	abstract = {Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.},
	urldate = {2023-07-11},
	journal = {Frontiers in Neuroscience},
	author = {Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},
	year = {2017},
	keywords = {LI neuron is basically ReLU},
}

@article{liu_spiking_2023,
	title = {Spiking {Neural}-{Networks}-{Based} {Data}-{Driven} {Control}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/12/2/310},
	doi = {10.3390/electronics12020310},
	abstract = {Machine learning can be effectively applied in control loops to make optimal control decisions robustly. There is increasing interest in using spiking neural networks (SNNs) as the apparatus for machine learning in control engineering because SNNs can potentially offer high energy efficiency, and new SNN-enabling neuromorphic hardware is being rapidly developed. A defining characteristic of control problems is that environmental reactions and delayed rewards must be considered. Although reinforcement learning (RL) provides the fundamental mechanisms to address such problems, implementing these mechanisms in SNN learning has been underexplored. Previously, spike-timing-dependent plasticity learning schemes (STDP) modulated by factors of temporal difference (TD-STDP) or reward (R-STDP) have been proposed for RL with SNN. Here, we designed and implemented an SNN controller to explore and compare these two schemes by considering cart-pole balancing as a representative example. Although the TD-based learning rules are very general, the resulting model exhibits rather slow convergence, producing noisy and imperfect results even after prolonged training. We show that by integrating the understanding of the dynamics of the environment into the reward function of R-STDP, a robust SNN-based controller can be learned much more efficiently than TD-STDP.},
	language = {en},
	number = {2},
	urldate = {2023-07-07},
	journal = {Electronics},
	author = {Liu, Yuxiang and Pan, Wei},
	month = jan,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Value function with SNN, control, reinforcement learning, spiking neural network},
	pages = {310},
}

@article{mei_zigzag_2021,
	title = {{ZigZag}: {Enlarging} {Joint} {Architecture}-{Mapping} {Design} {Space} {Exploration} for {DNN} {Accelerators}},
	volume = {70},
	issn = {1557-9956},
	shorttitle = {{ZigZag}},
	doi = {10.1109/TC.2021.3059962},
	abstract = {Building efficient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataflow. However, owing to the large joint design space, finding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efficient solutions are found compared to other SotAs, due to ZigZag's uneven mapping capabilities.},
	number = {8},
	journal = {IEEE Transactions on Computers},
	author = {Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
	month = aug,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Analytical models, Computer architecture, DNN, Hardware, Neural networks, Search engines, Search problems, Space exploration, accelerator, analytical model, dataflow, design space exploration, mapping, memory hierarchy, scheduling},
	pages = {1160--1174},
}

@misc{fedus_revisiting_2020,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	url = {https://arxiv.org/pdf/2007.06700.pdf},
	abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay -- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
	urldate = {2023-06-06},
	publisher = {arXiv},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	month = jul,
	year = {2020},
	note = {arXiv:2007.06700 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difﬁculty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ﬁrst challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
	language = {en},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv:1506.02438 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, loss function},
}

@article{scherr_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-17236-y},
	doi = {10.1038/s41467-020-17236-y},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Nature Communications},
	author = {Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models, Neuroscience, Read, Synaptic plasticity},
	pages = {3625},
}

@article{de_queiroz_reinforcement_2006,
	series = {Neural {Networks}},
	title = {Reinforcement learning of a simple control task using the spike response model},
	volume = {70},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231206002001},
	doi = {10.1016/j.neucom.2006.07.002},
	abstract = {In this work, we propose a variation of a direct reinforcement learning algorithm, suitable for usage with spiking neurons based on the spike response model (SRM). The SRM is a biologically inspired, flexible model of spiking neuron based on kernel functions that describe the effect of spike reception and emission on the membrane potential of the neuron. In our experiments, the spikes emitted by a SRM neuron are used as input signals in a simple control task. The reinforcement signal obtained from the environment is used by the direct reinforcement learning algorithm, that modifies the synaptic weights of the neuron, adjusting the spiking firing times in order to obtain a better performance at the given problem. The obtained results are comparable to those from classic methods based on value function approximation and temporal difference, for simple control tasks.},
	language = {en},
	number = {1},
	urldate = {2023-06-07},
	journal = {Neurocomputing},
	author = {de Queiroz, Murilo Saraiva and de Berrêdo, Roberto Coelho and de Pádua Braga, Antônio},
	month = dec,
	year = {2006},
	keywords = {Reinforcement learning, Spike response model, Spiking neuron},
	pages = {14--20},
}

@article{ding_chen_deep_2022,
	title = {Deep {Reinforcement} {Learning} with {Spiking} {Q}-learning},
	abstract = {With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of nonspiking neurons as the representation of Q-value, which can directly learn robust policies from highdimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN.},
	journal = {arXiv.org},
	author = {{Ding Chen} and {Peixi Peng} and {Tiejun Huang} and {Yonghong Tian}},
	year = {2022},
	note = {ARXIV\_ID: 2201.09754
S2ID: 0a3104f2ca2308ac9930dd57cfbbe112d04f841d},
	keywords = {read},
}

@misc{noauthor_co_nodate,
	title = {C\&{O} project - {OneDrive}},
	url = {https://tud365-my.sharepoint.com/personal/korneelvandenb_tudelft_nl//_layouts/15/onedrive.aspx?login_hint=korneelvandenb%40tudelft%2Enl&id=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project%2FRL%20of%20control%20task%20with%20spike%20response%2Epdf&parent=%2Fpersonal%2Fkorneelvandenb%5Ftudelft%5Fnl%2FDocuments%2FTuDelft%2FMSc1%2FQ4%2FC%26O%20project},
	urldate = {2023-06-07},
	keywords = {not read},
}

@misc{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv:1602.01783 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning},
}

@article{ha_world_2018,
	title = {World {Models}},
	url = {http://arxiv.org/abs/1803.10122},
	doi = {10.5281/zenodo.1207631},
	abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
	urldate = {2023-06-07},
	author = {Ha, David and Schmidhuber, Jürgen},
	month = mar,
	year = {2018},
	note = {arXiv:1803.10122 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{florian_reinforcement_2005,
	title = {A reinforcement learning algorithm for spiking neural networks},
	doi = {10.1109/synasc.2005.13},
	abstract = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
	journal = {Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
	author = {Florian, Răzvan V.},
	month = sep,
	year = {2005},
	doi = {10.1109/synasc.2005.13},
	note = {MAG ID: 2120905747
S2ID: cb6442b823c13339446a21fcb089428ec521a34c},
	pages = {299--306},
}

@article{bellec_biologically_2019,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets.},
	volume = {2019},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	journal = {arXiv: Neural and Evolutionary Computing},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jan,
	year = {2019},
	note = {ARXIV\_ID: 1901.09049
MAG ID: 2913168413
S2ID: ffd0f679a631b733ba2f779ce1aa0e5bbde3a8b0},
	pages = {1--37},
}

@article{liu_human-level_2022,
	title = {Human-{Level} {Control} {Through} {Directly} {Trained} {Deep} {Spiking} \${Q}\$-{Networks}},
	doi = {10.1109/tcyb.2022.3198259},
	abstract = {As the third-generation neural networks, spiking neural networks (SNNs) have great potential on neuromorphic hardware because of their high energy efficiency. However, deep spiking reinforcement learning (DSRL), that is, the reinforcement learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the nondifferentiable property of the spiking function. To address these issues, we propose a deep spiking {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DSQN) in this article. Specifically, we propose a directly trained DSRL architecture based on the leaky integrate-and-fire (LIF) neurons and deep {\textless}inline-formula xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater} {\textless}tex-math notation="LaTeX"{\textgreater}\$Q\${\textless}/tex-math{\textgreater} {\textless}/inline-formula{\textgreater} -network (DQN). Then, we adapt a direct spiking learning algorithm for the DSQN. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, generalization and energy efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly trained SNN.},
	journal = {IEEE transactions on cybernetics},
	author = {Liu, Guisong and {Wenjie Deng} and Xie, Xiurui and {Li Huang} and Tang, Huajin},
	month = jan,
	year = {2022},
	doi = {10.1109/tcyb.2022.3198259},
	pmid = {36063509},
	note = {ARXIV\_ID: 2201.07211
MAG ID: 4294691690
S2ID: 2190a17a5e937c065adc5c139b563026ac174136},
	pages = {1--12},
}

@article{luca_zanatta_artificial_2022,
	title = {Artificial versus spiking neural networks for reinforcement learning in {UAV} obstacle avoidance},
	doi = {10.1145/3528416.3530865},
	abstract = {Spiking Neural Networks (SNN) are gaining more interest from the scientific community thanks to the promise of greater energy-efficient and greater computational power. This poses several challenges as today's SNN training for RL is based on Artificial Neural Network (ANN) training and then conversion from ANN to SNN, which does not leverage SNN event-based processing inherent capabilities. The present work compares an ANN and an SNN in an event-camera-based obstacle avoidance task, trained with Reinforcement Learning (RL) using the Deep Q-Learning (DQL) algorithm. We create an experimental setup composed of Unreal Engine 4, AirSim, and an event camera that simulates a real-world obstacle avoidance environment. Additionally, we train an SNN with a gradient-based training method enabling the use of all their expressiveness even in the training phase, showing comparable performance between the ANN and the SNN. To the best of our knowledge, we are the first that implements an entire realistic pipeline with a photo-realistic simulator (Airsim) and train an SNN without converting it from a pre-trained ANN.},
	journal = {ACM International Conference on Computing Frontiers},
	author = {{Luca Zanatta} and {Francesco Barchi} and {Andrea Bartolini} and {Andrea Acquaviva}},
	month = may,
	year = {2022},
	doi = {10.1145/3528416.3530865},
	note = {MAG ID: 4229040499
S2ID: 0903d078a954427d8c875da922d52d187981b958},
}

@article{seung_learning_2003,
	title = {Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.},
	volume = {40},
	doi = {10.1016/s0896-6273(03)00761-x},
	abstract = {Abstract  It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are "hedonistic," responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
	number = {6},
	journal = {Neuron},
	author = {Seung, H. Sebastian},
	month = dec,
	year = {2003},
	doi = {10.1016/s0896-6273(03)00761-x},
	pmid = {14687542},
	note = {MAG ID: 2061897041},
	pages = {1063--1073},
}

@article{g_bellec_supplementary_2019,
	title = {Supplementary materials for: {A} solution to the learning dilemma for recurrent networks of spiking neurons},
	abstract = {S2 Optimization and regularization procedures 4 S2.1 Optimization procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.2 Firing rate regularization for LSNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 S2.3 Weight decay regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 S2.4 Optimization with rewiring for sparse network connectivity . . . . . . . . . . . . . . . . . . 5},
	author = {{G. Bellec} and {Franz Scherr} and {Anand Subramoney} and {Elias Hajek} and {Darjan Salaj} and {R. Legenstein} and {W. Maass}},
	year = {2019},
	note = {S2ID: b3d964aa6bba3358b6921b36a19bae6454871498},
}

@article{bellec_solution_2019,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {2019},
	doi = {10.1101/738385},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. But in spite of extensive research, it has remained open how they can learn through synaptic plasticity to carry out complex network computations. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A new mathematical insight tells us how these pieces need to be combined to enable biologically plausible online  network learning through gradient descent, in particular deep reinforcement learning. This new learning method -- called e-prop -- approaches the performance of BPTT (backpropagation through time), the best known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in novel energy-efficient spike-based hardware for AI.},
	journal = {bioRxiv},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = aug,
	year = {2019},
	doi = {10.1101/738385},
	pmcid = {7367848},
	pmid = {32681001},
	note = {MAG ID: 2967417697
S2ID: 858549b00245aadc92f91a2540f01398f5f389ae},
	pages = {738385},
}

@article{kapturowski_recurrent_2019,
	title = {{RECURRENT} {EXPERIENCE} {REPLAY} {IN} {DISTRIBUTED} {REINFORCEMENT} {LEARNING}},
	abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and ﬁxed set of hyperparameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the ﬁrst agent to exceed human-level performance in 52 of the 57 Atari games.},
	language = {en},
	author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
	year = {2019},
}

@misc{bellec_biologically_2019-1,
	title = {Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets},
	url = {http://arxiv.org/abs/1901.09049},
	abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = feb,
	year = {2019},
	note = {arXiv:1901.09049 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@misc{A3C,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}