@inproceedings{
madaan2023learning,
title={Learning Performance-Improving Code Edits},
author={Madaan, Aman and Shypula, Alexander and Alon, Uri and Hashemi, Milad and Ranganathan, Parthasarathy and Yang, Yiming and Neubig, Graham and Yazdanbakhsh, Amir},
booktitle={Submitted to The Twelfth International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=ix7rLVHXyY},
note={under review}
}
@misc{yik2024neurobench,
      title={NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems}, 
      author={Jason Yik and Korneel Van den Berghe et al.},
      year={2024},
      eprint={2304.04640},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{li2023large,
  title={Large Language Model-Aware In-Context Learning for Code Generation},
  author={Li, Jia and Li, Ge and Tao, Chongyang and Zhang, Huangzhao and Liu, Fang and Jin, Zhi},
  journal={arXiv preprint arXiv:2310.09748},
  year={2023}
}

@misc{alpha2023,
	author = {{AlphaCode Team}},
	title = {{AlphaCode 2 Technical Report}},
	howpublished = {\url{https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf}},
	year = {2023},
	note = {[Accessed 07-12-2023]},
}

@inproceedings{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew and Dasgupta, Ishita and Chan, Stephanie and Mathewson, Kory and Tessler, Mh and Creswell, Antonia and McClelland, James and Wang, Jane and Hill, Felix},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={537--563},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}

@misc{rozière2024code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18--20, 2019, Proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{zhou2023codebertscore,
  title={Codebertscore: Evaluating code generation with pretrained models of code},
  author={Zhou, Shuyan and Alon, Uri and Agarwal, Sumit and Neubig, Graham},
  journal={arXiv preprint arXiv:2302.05527},
  year={2023}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}
@article{liu2020retrieval,
  title={Retrieval-augmented generation for code summarization via hybrid gnn},
  author={Liu, Shangqing and Chen, Yu and Xie, Xiaofei and Siow, Jingkai and Liu, Yang},
  journal={arXiv preprint arXiv:2006.05405},
  year={2020}
}
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}
% humaneval
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
% humaneval+
@article{liu2023your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={arXiv preprint arXiv:2305.01210},
  year={2023}
}
@article{poesia2022synchromesh,
  title={Synchromesh: Reliable code generation from pre-trained language models},
  author={Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  journal={arXiv preprint arXiv:2201.11227},
  year={2022}
}
@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@inproceedings{baevski2023efficient,
  title={Efficient self-supervised learning with contextualized target representations for vision, speech and language},
  author={Baevski, Alexei and Babu, Arun and Hsu, Wei-Ning and Auli, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1416--1429},
  year={2023},
  organization={PMLR}
}
@inproceedings{girdhar2023omnimae,
  title={Omnimae: Single model masked pretraining on images and videos},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10406--10417},
  year={2023}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12104--12113},
  year={2022}
}

@misc{kanade2020learning,
      title={Learning and Evaluating Contextual Embedding of Source Code}, 
      author={Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},
      year={2020},
      eprint={2001.00059},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{feng2020codebert,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{husain2020codesearchnet,
      title={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search}, 
      author={Hamel Husain and Ho-Hsiang Wu and Tiferet Gazit and Miltiadis Allamanis and Marc Brockschmidt},
      year={2020},
      eprint={1909.09436},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{guo2021graphcodebert,
      title={GraphCodeBERT: Pre-training Code Representations with Data Flow}, 
      author={Daya Guo and Shuo Ren and Shuai Lu and Zhangyin Feng and Duyu Tang and Shujie Liu and Long Zhou and Nan Duan and Alexey Svyatkovskiy and Shengyu Fu and Michele Tufano and Shao Kun Deng and Colin Clement and Dawn Drain and Neel Sundaresan and Jian Yin and Daxin Jiang and Ming Zhou},
      year={2021},
      eprint={2009.08366},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{wang2021syncobert,
      title={SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation}, 
      author={Xin Wang and Yasheng Wang and Fei Mi and Pingyi Zhou and Yao Wan and Xiao Liu and Li Li and Hao Wu and Jin Liu and Xin Jiang},
      year={2021},
      eprint={2108.04556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2021codet5,
      title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
      author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
      year={2021},
      eprint={2109.00859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{clement-etal-2020-pymt5,
    title = "{P}y{MT}5: multi-mode translation of natural language and Python code with transformers",
    author = "Clement, Colin  and
      Drain, Dawn  and
      Timcheck, Jonathan  and
      Svyatkovskiy, Alexey  and
      Sundaresan, Neel",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.728",
    doi = "10.18653/v1/2020.emnlp-main.728",
    pages = "9052--9065",
    abstract = "Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1{\%} syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",
}

@inproceedings{ahmad-etal-2021-unified,
    title = "Unified Pre-training for Program Understanding and Generation",
    author = "Ahmad, Wasi  and
      Chakraborty, Saikat  and
      Ray, Baishakhi  and
      Chang, Kai-Wei",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.211",
    doi = "10.18653/v1/2021.naacl-main.211",
    pages = "2655--2668",
    abstract = "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART{'}s effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., {``}if{``} block inside an {``}else{``} block is equivalent to {``}else if{``} block) that are crucial to program semantics and thus excels even with limited annotations.",
}

@article{Li_2022,
   title={Competition-level code generation with AlphaCode},
   volume={378},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.abq1158},
   DOI={10.1126/science.abq1158},
   number={6624},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
   year={2022},
   month=dec, pages={1092–1097} }

@misc{chakraborty2022natgen,
      title={NatGen: Generative pre-training by "Naturalizing" source code}, 
      author={Saikat Chakraborty and Toufique Ahmed and Yangruibo Ding and Premkumar Devanbu and Baishakhi Ray},
      year={2022},
      eprint={2206.07585},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{wang2023codet5plus,
      title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation}, 
      author={Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui and Junnan Li and Steven C. H. Hoi},
      year={2023},
      eprint={2305.07922},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lu2021codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      eprint={2102.04664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{xu2022systematic,
      title={A Systematic Evaluation of Large Language Models of Code}, 
      author={Frank F. Xu and Uri Alon and Graham Neubig and Vincent J. Hellendoorn},
      year={2022},
      eprint={2202.13169},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}
@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}
@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@misc{brandfonbrener2024verified,
      title={Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search}, 
      author={David Brandfonbrener and Sibi Raja and Tarun Prasad and Chloe Loughridge and Jianang Yang and Simon Henniger and William E. Byrd and Robert Zinkov and Nada Amin},
      year={2024},
      eprint={2402.08147},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{ishida2024langprop,
      title={LangProp: A code optimization framework using Language Models applied to driving}, 
      author={Shu Ishida and Gianluca Corrado and George Fedoseev and Hudson Yeo and Lloyd Russell and Jamie Shotton and João F. Henriques and Anthony Hu},
      year={2024},
      eprint={2401.10314},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{cummins2023large,
      title={Large Language Models for Compiler Optimization}, 
      author={Chris Cummins and Volker Seeker and Dejan Grubisic and Mostafa Elhoushi and Youwei Liang and Baptiste Roziere and Jonas Gehring and Fabian Gloeckle and Kim Hazelwood and Gabriel Synnaeve and Hugh Leather},
      year={2023},
      eprint={2309.07062},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{zheng2024survey,
      title={A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends}, 
      author={Zibin Zheng and Kaiwen Ning and Yanlin Wang and Jingwen Zhang and Dewu Zheng and Mingxi Ye and Jiachi Chen},
      year={2024},
      eprint={2311.10372},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{bigcodecollaboration2023bigcode,
      title={The BigCode Project Governance Card}, 
      author={BigCode collaboration and Sean Hughes and Harm de Vries and Jennifer Robinson and Carlos Muñoz Ferrandis and Loubna Ben Allal and Leandro von Werra and Jennifer Ding and Sebastien Paquet and Yacine Jernite},
      year={2023},
      eprint={2312.03872},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{FortranArchive,
  author = {Computer History Museum},
  title = {The Fortran Automatic Coding System},
  howpublished = {\url{https://archive.computerhistory.org/resources/text/Fortran/102663113.05.01.acc.pdf}},
  year = {1966},
  note = {Accessed on: 2024-03-18}
}

@misc{CArchive,
  author = {JTC1/SC22/WG14 - C},
  title = {The C programming langauge},
  howpublished = {\url{https://www.open-std.org/jtc1/sc22/wg14/}},
  year = {1966},
  note = {Accessed on: 2024-03-18}
}

@misc{C++Archive,
  author = {ISO},
  title = {The C++ standard},
  howpublished = {\url{https://isocpp.org/std/the-standard}},
  year = {1966},
  note = {Accessed on: 2024-03-18}
}

@inproceedings{Sevilla_2022,
   title={Compute Trends Across Three Eras of Machine Learning},
   url={http://dx.doi.org/10.1109/IJCNN55064.2022.9891914},
   DOI={10.1109/ijcnn55064.2022.9891914},
   booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
   publisher={IEEE},
   author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
   year={2022},
   month=jul }

@misc{dao2022flashattention,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{hao2023reasoning,
      title={Reasoning with Language Model is Planning with World Model}, 
      author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
      year={2023},
      eprint={2305.14992},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{duan2023leveraging,
      title={Leveraging Reinforcement Learning and Large Language Models for Code Optimization}, 
      author={Shukai Duan and Nikos Kanakaris and Xiongye Xiao and Heng Ping and Chenyu Zhou and Nesreen K. Ahmed and Guixiang Ma and Mihai Capota and Theodore L. Willke and Shahin Nazarian and Paul Bogdan},
      year={2023},
      eprint={2312.05657},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{defogsqlcoder,
title={Defog.ai - Fine-tuned AI models for enterprise SQL}, url={https://defog.ai/blog/open-sourcing-sqlcoder/}, journal={Defog.ai - Fine-tuned AI models for enterprise SQL}} 

@misc{li2023reinforcement,
      title={Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism}, 
      author={Zihao Li and Zhuoran Yang and Mengdi Wang},
      year={2023},
      eprint={2305.18438},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{puri2021codenet,
      title={CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks}, 
      author={Ruchir Puri and David S. Kung and Geert Janssen and Wei Zhang and Giacomo Domeniconi and Vladimir Zolotov and Julian Dolby and Jie Chen and Mihir Choudhury and Lindsey Decker and Veronika Thost and Luca Buratti and Saurabh Pujar and Shyam Ramji and Ulrich Finkler and Susan Malaika and Frederick Reiss},
      year={2021},
      eprint={2105.12655},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}